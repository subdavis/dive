{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DIVE Documentation This is the documentation site for DIVE, a free and open-source annotation and analysis platform for web and desktop built by Kitware . DIVE integrates with the VIAME toolkit , but it can also be used on its own. Try the web version Get the desktop app Get support Feature Comparison Web Desktop Data Support Load your own images and videos \u2714\ufe0f \u2714\ufe0f Image and video transcoding \u2714\ufe0f \u2714\ufe0f Import using image lists \u274c \u2714\ufe0f Load annotations from supported formats \u2714\ufe0f \u2714\ufe0f Create new object and track annotation \u2714\ufe0f \u2714\ufe0f Annotation export \u2714\ufe0f \u2714\ufe0f Dataset export for portability between web and desktop \u2714\ufe0f \u2714\ufe0f Permissions and sharing support for team collaboration \u2714\ufe0f \u274c Annotation Support Bounding boxes \u2714\ufe0f \u2714\ufe0f Polygons \u2714\ufe0f \u2714\ufe0f Head/Tail lines \u2714\ufe0f \u2714\ufe0f Linear interpolation \u2714\ufe0f \u2714\ufe0f Track split \u2714\ufe0f \u2714\ufe0f Multi-way track merge \u2714\ufe0f \u2714\ufe0f VIAME Integration Support Run VIAME object detection and tracking \u2714\ufe0f \u2714\ufe0f Run VIAME detector and tracker training \u2714\ufe0f \u2714\ufe0f VIAME multi-camera pipelines \u274c \u2714\ufe0f Manual refinement of auto-generated annotations \u2714\ufe0f \u2714\ufe0f Concepts and Definitions DIVE is the annotator and data management software system. It is our name for the code and capabilities, including both web and desktop, that can be deployed and configured for a variety of needs. VIAME (Video and Image Analytics for Marine Environments) is a suite of computer vision tools for object detection, tracking, rapid model generation, and many other types of analysis. Get more info at viametoolkit.org VIAME Web is the specific DIVE Web deployment at viame.kitware.com . It includes a web-based annotator with the capabilities to run VIAME workflows on user-provided data. You may deploy the web system into your own lab or cloud environment. Detection - A single annotation. A detection could be associated with a point in time within a track, or it could have no temporal association. Features - Bounding box, polygon, head/tail points or other visible elements of a detection. Track - A collection of detections spanned over multiple frames in a video or image sequence. Tracks include a start and end time and can have gap periods in which no detections exist. Types - Every track (or detection, if tracks aren't applicable) has one or more types that should be used to annotate the primary characteristic you are interested in classifying. Types are typically used to train a single or multi-class classifier. A track (or detection) may have multiple types with confidence values associated. Frame - A single image or point in time for a video or image sequence. Key Frame - Every manually drawn annotation is considered a keyframe, and all automated pipelines produce keyframes. Only keyframes can have attributes. Key frame detections are differentiated from interpolated detections, which are the implicit bounding boxes you see when linear interpolation is enabled. Interpolation - The implicit bounding boxes between keyframes in a track. Attributes - Attributes are free-form secondary characteristics on both tracks and detections. For example, a fish type track may have an is_adult boolean attribute.","title":"Home"},{"location":"#dive-documentation","text":"This is the documentation site for DIVE, a free and open-source annotation and analysis platform for web and desktop built by Kitware . DIVE integrates with the VIAME toolkit , but it can also be used on its own. Try the web version Get the desktop app Get support","title":"DIVE Documentation"},{"location":"#feature-comparison","text":"Web Desktop Data Support Load your own images and videos \u2714\ufe0f \u2714\ufe0f Image and video transcoding \u2714\ufe0f \u2714\ufe0f Import using image lists \u274c \u2714\ufe0f Load annotations from supported formats \u2714\ufe0f \u2714\ufe0f Create new object and track annotation \u2714\ufe0f \u2714\ufe0f Annotation export \u2714\ufe0f \u2714\ufe0f Dataset export for portability between web and desktop \u2714\ufe0f \u2714\ufe0f Permissions and sharing support for team collaboration \u2714\ufe0f \u274c Annotation Support Bounding boxes \u2714\ufe0f \u2714\ufe0f Polygons \u2714\ufe0f \u2714\ufe0f Head/Tail lines \u2714\ufe0f \u2714\ufe0f Linear interpolation \u2714\ufe0f \u2714\ufe0f Track split \u2714\ufe0f \u2714\ufe0f Multi-way track merge \u2714\ufe0f \u2714\ufe0f VIAME Integration Support Run VIAME object detection and tracking \u2714\ufe0f \u2714\ufe0f Run VIAME detector and tracker training \u2714\ufe0f \u2714\ufe0f VIAME multi-camera pipelines \u274c \u2714\ufe0f Manual refinement of auto-generated annotations \u2714\ufe0f \u2714\ufe0f","title":"Feature Comparison"},{"location":"#concepts-and-definitions","text":"DIVE is the annotator and data management software system. It is our name for the code and capabilities, including both web and desktop, that can be deployed and configured for a variety of needs. VIAME (Video and Image Analytics for Marine Environments) is a suite of computer vision tools for object detection, tracking, rapid model generation, and many other types of analysis. Get more info at viametoolkit.org VIAME Web is the specific DIVE Web deployment at viame.kitware.com . It includes a web-based annotator with the capabilities to run VIAME workflows on user-provided data. You may deploy the web system into your own lab or cloud environment. Detection - A single annotation. A detection could be associated with a point in time within a track, or it could have no temporal association. Features - Bounding box, polygon, head/tail points or other visible elements of a detection. Track - A collection of detections spanned over multiple frames in a video or image sequence. Tracks include a start and end time and can have gap periods in which no detections exist. Types - Every track (or detection, if tracks aren't applicable) has one or more types that should be used to annotate the primary characteristic you are interested in classifying. Types are typically used to train a single or multi-class classifier. A track (or detection) may have multiple types with confidence values associated. Frame - A single image or point in time for a video or image sequence. Key Frame - Every manually drawn annotation is considered a keyframe, and all automated pipelines produce keyframes. Only keyframes can have attributes. Key frame detections are differentiated from interpolated detections, which are the implicit bounding boxes you see when linear interpolation is enabled. Interpolation - The implicit bounding boxes between keyframes in a track. Attributes - Attributes are free-form secondary characteristics on both tracks and detections. For example, a fish type track may have an is_adult boolean attribute.","title":"Concepts and Definitions"},{"location":"Annotation-QuickStart/","text":"Annotation Quickstart Before following the quickstart, it could be helpful to skim the User Interface Guide Single Frame Detections How to quickly create multiple detections on a single image frame. Click (creation settings menu) in the Track List area. From the Mode dropdown, choose Detection . From the Type dropdown, choose or enter a default name that all new detections will have. If the type doesn't exist yet, enter a name to create a new one. Turn on the Continuous Mode switch if you would like to automatically re-enter the creation state so you can click-and-drag repeatedly to quickly create many detections. Enter the annotation creation state by clicking Detection or pressing the N key. Create your first detection by clicking and dragging to draw a rectangle. If you are in continuous mode, click and drag again to create the next detection. Press Esc to exit continuous creation mode. Single Detection Mode Demo The demo below shows how to use Detection mode to quickly create numerous detections of the same type. Track Annotations How to quickly create track annotations for a video or image sequence. Interpolation Mode Linear interpolation is a kind of spatio-temporal annotation that allows the inference of bounding boxes between keyframes. Interpolation mode is the fastest and easiest way to generate track annotations. Interpolation editing for existing tracks will only be enabled on tracks that span more than one frame. It is enabled on new tracks by default. Click (creation settings menu) in the Track List area. From the Mode dropdown, choose Track . Also ensure that the Interpolate switch is turned on . Enter the annotation creation state by clicking Track or pressing the N key. Create your first detection by clicking and dragging to draw a rectangle around the object you want to track. You can now go forward one or more frames by pressing F or Right or by using the Timeline controls and an outline of the previous annotation will remain. To set another keyframe, either move or resize the transparent annotation or press K . There are also controls on for the currently selected track to add/remove keyframes. and will allow you to add and remove the current keyframe. and will turn on or off interpolation for the current keyframe interval region you are in. Visualizing interpolated tracks Click Events in the Timeline controls to see where interpolation occurs and where the keyframes are located. Keyframes are indicated by solid rectangular blue tick marks in the highlighted track. Interpolated regions are indicated by a thin yellow line between keyframes. Gap regions are indicated by areas with neither interpolated frames nor keyframes. Typically means that a track is off-camera or occluded. Interpolation Mode Demo Advance Frame Mode This mode keeps you editing the same track while automatically advancing the frame each time a detection is drawn. In most cases interpolation mode will be easier . Click (creation settings menu) in the Track List area. From the Mode dropdown, choose Track . Also ensure that the Interpolate switch is turned off . Enter the annotation creation state by clicking Track or pressing the N key. Create your first detection by clicking and dragging to draw a rectangle around the object you want to track. Now each time an individual detection is drawn the frame will automatically advance to the next frame. Press Esc to end creation of the track. Advance Frame Mode Demo The demo below shows how to use AdvanceFrame mode to travel through the video while creating annotations. Head Tail Annotations Adding Head/Tail points to existing annotations Right-click an existing detection to enter edit mode. Enter head/tail creation mode In the Edit bar , click Or Press H to create a head point. Or press T to create a tail point. The mouse cursor will become a crosshair. Click in the annotator to place each point. Once the first marker is placed it automatically transitions to the second marker. If you start with head, the second one will be the tail and vice versa. Creating new annotations using Head/Tail points You can create a track by starting with a head/tail annotation or just a single point. Enter the annotation creation state by clicking Track or pressing the N key. In the Edit bar , click to switch to head/tail creation mode or press H , T , or 3 . The mouse cursor will become a crosshair. Click in the annotator to place each point. Press Esc to finish creation after one or both points have been placed. Other notes on Head/Tail The head point is denoted by a filled circle, while the tail point is denoted by a hollow circle. You don't have to place both markers. Press Esc on your keyboard at anytime to exit out of the line creation mode. You can modify an existing head/tail marker by placing the annotation into 'Edit Mode' and then selecting the line tool from the editing options. You can delete a head/tail pair by selecting a detection with existing markers, entering edit mode, and clicking Delete Linestring Fish Head Tail Demo Polygon Annotations Every track is required to have a bounding box, but a polygon region may be added. When a polygon is created or edited it will generate or adjust the bounding box to fit the size of the polygon. Polygon Creation Enter the annotation creation state by clicking Track or pressing the N key. In the Edit bar , click or press 2 to enter polygon creation mode. Place each point on the polygon by clicking. Right-Click to automatically close the polygon or press Esc to cancel creation. Polygon Editing Right click an annotation to enter edit mode. In the Edit bar , click or press 2 to enter polygon edit mode. Click and drag any large circle handle to move it. This will move the point to a new position and recalculate the bounding box. Click and drag any small circle handle to create new points. This can be used to adjust the polygon and make it appear smoother. To delete the whole polygon , in the Edit bar , click Del polygon To delete a single keypoint , click its handle then click Del Point N Polygon Demo","title":"Annotation Quickstart"},{"location":"Annotation-QuickStart/#annotation-quickstart","text":"Before following the quickstart, it could be helpful to skim the User Interface Guide","title":"Annotation Quickstart"},{"location":"Annotation-QuickStart/#single-frame-detections","text":"How to quickly create multiple detections on a single image frame. Click (creation settings menu) in the Track List area. From the Mode dropdown, choose Detection . From the Type dropdown, choose or enter a default name that all new detections will have. If the type doesn't exist yet, enter a name to create a new one. Turn on the Continuous Mode switch if you would like to automatically re-enter the creation state so you can click-and-drag repeatedly to quickly create many detections. Enter the annotation creation state by clicking Detection or pressing the N key. Create your first detection by clicking and dragging to draw a rectangle. If you are in continuous mode, click and drag again to create the next detection. Press Esc to exit continuous creation mode.","title":"Single Frame Detections"},{"location":"Annotation-QuickStart/#single-detection-mode-demo","text":"The demo below shows how to use Detection mode to quickly create numerous detections of the same type.","title":"Single Detection Mode Demo"},{"location":"Annotation-QuickStart/#track-annotations","text":"How to quickly create track annotations for a video or image sequence.","title":"Track Annotations"},{"location":"Annotation-QuickStart/#interpolation-mode","text":"Linear interpolation is a kind of spatio-temporal annotation that allows the inference of bounding boxes between keyframes. Interpolation mode is the fastest and easiest way to generate track annotations. Interpolation editing for existing tracks will only be enabled on tracks that span more than one frame. It is enabled on new tracks by default. Click (creation settings menu) in the Track List area. From the Mode dropdown, choose Track . Also ensure that the Interpolate switch is turned on . Enter the annotation creation state by clicking Track or pressing the N key. Create your first detection by clicking and dragging to draw a rectangle around the object you want to track. You can now go forward one or more frames by pressing F or Right or by using the Timeline controls and an outline of the previous annotation will remain. To set another keyframe, either move or resize the transparent annotation or press K . There are also controls on for the currently selected track to add/remove keyframes. and will allow you to add and remove the current keyframe. and will turn on or off interpolation for the current keyframe interval region you are in.","title":"Interpolation Mode"},{"location":"Annotation-QuickStart/#visualizing-interpolated-tracks","text":"Click Events in the Timeline controls to see where interpolation occurs and where the keyframes are located. Keyframes are indicated by solid rectangular blue tick marks in the highlighted track. Interpolated regions are indicated by a thin yellow line between keyframes. Gap regions are indicated by areas with neither interpolated frames nor keyframes. Typically means that a track is off-camera or occluded.","title":"Visualizing interpolated tracks"},{"location":"Annotation-QuickStart/#interpolation-mode-demo","text":"","title":"Interpolation Mode Demo"},{"location":"Annotation-QuickStart/#advance-frame-mode","text":"This mode keeps you editing the same track while automatically advancing the frame each time a detection is drawn. In most cases interpolation mode will be easier . Click (creation settings menu) in the Track List area. From the Mode dropdown, choose Track . Also ensure that the Interpolate switch is turned off . Enter the annotation creation state by clicking Track or pressing the N key. Create your first detection by clicking and dragging to draw a rectangle around the object you want to track. Now each time an individual detection is drawn the frame will automatically advance to the next frame. Press Esc to end creation of the track.","title":"Advance Frame Mode"},{"location":"Annotation-QuickStart/#advance-frame-mode-demo","text":"The demo below shows how to use AdvanceFrame mode to travel through the video while creating annotations.","title":"Advance Frame Mode Demo"},{"location":"Annotation-QuickStart/#head-tail-annotations","text":"","title":"Head Tail Annotations"},{"location":"Annotation-QuickStart/#adding-headtail-points-to-existing-annotations","text":"Right-click an existing detection to enter edit mode. Enter head/tail creation mode In the Edit bar , click Or Press H to create a head point. Or press T to create a tail point. The mouse cursor will become a crosshair. Click in the annotator to place each point. Once the first marker is placed it automatically transitions to the second marker. If you start with head, the second one will be the tail and vice versa.","title":"Adding Head/Tail points to existing annotations"},{"location":"Annotation-QuickStart/#creating-new-annotations-using-headtail-points","text":"You can create a track by starting with a head/tail annotation or just a single point. Enter the annotation creation state by clicking Track or pressing the N key. In the Edit bar , click to switch to head/tail creation mode or press H , T , or 3 . The mouse cursor will become a crosshair. Click in the annotator to place each point. Press Esc to finish creation after one or both points have been placed.","title":"Creating new annotations using Head/Tail points"},{"location":"Annotation-QuickStart/#other-notes-on-headtail","text":"The head point is denoted by a filled circle, while the tail point is denoted by a hollow circle. You don't have to place both markers. Press Esc on your keyboard at anytime to exit out of the line creation mode. You can modify an existing head/tail marker by placing the annotation into 'Edit Mode' and then selecting the line tool from the editing options. You can delete a head/tail pair by selecting a detection with existing markers, entering edit mode, and clicking Delete Linestring","title":"Other notes on Head/Tail"},{"location":"Annotation-QuickStart/#fish-head-tail-demo","text":"","title":"Fish Head Tail Demo"},{"location":"Annotation-QuickStart/#polygon-annotations","text":"Every track is required to have a bounding box, but a polygon region may be added. When a polygon is created or edited it will generate or adjust the bounding box to fit the size of the polygon.","title":"Polygon Annotations"},{"location":"Annotation-QuickStart/#polygon-creation","text":"Enter the annotation creation state by clicking Track or pressing the N key. In the Edit bar , click or press 2 to enter polygon creation mode. Place each point on the polygon by clicking. Right-Click to automatically close the polygon or press Esc to cancel creation.","title":"Polygon Creation"},{"location":"Annotation-QuickStart/#polygon-editing","text":"Right click an annotation to enter edit mode. In the Edit bar , click or press 2 to enter polygon edit mode. Click and drag any large circle handle to move it. This will move the point to a new position and recalculate the bounding box. Click and drag any small circle handle to create new points. This can be used to adjust the polygon and make it appear smoother. To delete the whole polygon , in the Edit bar , click Del polygon To delete a single keypoint , click its handle then click Del Point N","title":"Polygon Editing"},{"location":"Annotation-QuickStart/#polygon-demo","text":"","title":"Polygon Demo"},{"location":"Annotation-User-Interface-Overview/","text":"User Interface Guide Introduction This documentation section provides a reference guide to the annotation interface organized by screen region. Navigation and Editing Bar - Controls to return back to browser as well as perform higher level functions such as running pipelines. Save Button. Controls the viewing of annotations on screen and allows for the editing/creation of annotations. Annotation View - where the image/video is displayed as well as all annotations Type List - A list of all the types of tracks/detections on the page that can be used to filter the current view. Track List - List of all the tracks as well as providing a way to perform editing functions on those tracks. Timeline - timeline view of tracks and detections, as well as an interface to control the current frame along the video/image-sequence Attributes - Attributes panel used to assign attributes to individual tracks or detections.","title":"Introduction"},{"location":"Annotation-User-Interface-Overview/#user-interface-guide-introduction","text":"This documentation section provides a reference guide to the annotation interface organized by screen region. Navigation and Editing Bar - Controls to return back to browser as well as perform higher level functions such as running pipelines. Save Button. Controls the viewing of annotations on screen and allows for the editing/creation of annotations. Annotation View - where the image/video is displayed as well as all annotations Type List - A list of all the types of tracks/detections on the page that can be used to filter the current view. Track List - List of all the tracks as well as providing a way to perform editing functions on those tracks. Timeline - timeline view of tracks and detections, as well as an interface to control the current frame along the video/image-sequence Attributes - Attributes panel used to assign attributes to individual tracks or detections.","title":"User Interface Guide Introduction"},{"location":"Command-Line-Tools/","text":"DIVE Command Line Tools Note This page is not related to the VIAME command line (i.e. kwiver , viame_train_detector ) Some of the DIVE data conversion features are exposed through dive . Features Convert between various supported formats Verify the integrity of a DIVE Json annotation file. Installation Follow the docs in the Debug Utils and Command Line Tools section of server/README.md . 1 2 3 git clone https://github.com/Kitware/dive.git cd dive/server poetry install Usage 1 2 3 4 5 6 7 8 9 10 11 12 13 ~$ poetry run dive convert --help # Usage: dive convert [OPTIONS] COMMAND [ARGS]... # Options: # --version Show the version and exit. # --help Show this message and exit. # Commands: # coco2dive # dive2viame # kpf2dive # viame2dive","title":"Command Line Tools"},{"location":"Command-Line-Tools/#dive-command-line-tools","text":"Note This page is not related to the VIAME command line (i.e. kwiver , viame_train_detector ) Some of the DIVE data conversion features are exposed through dive .","title":"DIVE Command Line Tools"},{"location":"Command-Line-Tools/#features","text":"Convert between various supported formats Verify the integrity of a DIVE Json annotation file.","title":"Features"},{"location":"Command-Line-Tools/#installation","text":"Follow the docs in the Debug Utils and Command Line Tools section of server/README.md . 1 2 3 git clone https://github.com/Kitware/dive.git cd dive/server poetry install","title":"Installation"},{"location":"Command-Line-Tools/#usage","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 ~$ poetry run dive convert --help # Usage: dive convert [OPTIONS] COMMAND [ARGS]... # Options: # --version Show the version and exit. # --help Show this message and exit. # Commands: # coco2dive # dive2viame # kpf2dive # viame2dive","title":"Usage"},{"location":"DataFormats/","text":"Data Formats DIVE Desktop and Web support a number of annotation and configuration formats. The following formats can be uploaded or imported alongside your media and will be automatically parsed. DIVE Annotation JSON (default annotation format) DIVE Configuration JSON VIAME CSV COCO and KWCOCO KPF (KWIVER Packet Format) for MEVA DIVE Annotation JSON Files are typically named result_{dataset-name}.json . This JSON file is a map of numeric track identifiers to tracks, or Record<string, TrackData> , where TrackData is defined below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 interface Feature { frame : number ; flick? : Readonly < number > ; interpolate? : boolean ; keyframe? : boolean ; bounds ?: [ number , number , number , number ]; // [x1, y1, x2, y2] as (left, top), (bottom, right) geometry? : GeoJSON.FeatureCollection < GeoJSON . Point | GeoJSON . Polygon | GeoJSON . LineString | GeoJSON . Point > ; fishLength? : number ; attributes? : Record < string , unknown > ; head ?: [ number , number ]; tail ?: [ number , number ]; } interface TrackData { trackId : TrackId ; meta : Record < string , unknown > ; attributes : Record < string , unknown > ; confidencePairs : Array < [ string , number ] > ; features : Array < Feature > ; begin : number ; end : number ; } The source TrackData definition can be found here as a TypeScript interface. Example DIVE annotation file This is a relatively simple example, and many optional fields are not included. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"1\" : { \"trackId\" : 1 , \"meta\" : {}, \"attributes\" : {}, \"confidencePairs\" : [[ \"fish\" , 0.87 ], [ \"rock\" , 0.22 ]], \"features\" : [ { \"frame\" : 0 , \"bounds\" : [ 0 , 0 , 10 , 10 ], \"interpolate\" : true }, { \"frame\" : 2 , \"bounds\" : [ 10 , 10 , 20 , 20 ] }, ], \"begin\" : 0 , \"end\" : 2 , }, } DIVE Configuration JSON This information provides the specification for an individual dataset. It consists of the following. Allowed types (or labels) and their appearances are defined by customTypeStyling Preset confidence filters for those types are defined in confidenceFilters Track and Detection attribute specifications are defined in attributes The full DatasetMetaMutable definition can be found here . 1 2 3 4 5 6 interface DatasetMetaMutable { version : number ; customTypeStyling? : Record < string , CustomStyle > ; confidenceFilters? : Record < string , number > ; attributes? : Readonly < Record < string , Attribute >> ; } VIAME CSV Read the VIAME CSV Specification . COCO and KWCOCO Read the COCO Specification Read the KWCOCO Specification KWIVER Packet Format (KPF) Read the KPF Specification","title":"Data Formats"},{"location":"DataFormats/#data-formats","text":"DIVE Desktop and Web support a number of annotation and configuration formats. The following formats can be uploaded or imported alongside your media and will be automatically parsed. DIVE Annotation JSON (default annotation format) DIVE Configuration JSON VIAME CSV COCO and KWCOCO KPF (KWIVER Packet Format) for MEVA","title":"Data Formats"},{"location":"DataFormats/#dive-annotation-json","text":"Files are typically named result_{dataset-name}.json . This JSON file is a map of numeric track identifiers to tracks, or Record<string, TrackData> , where TrackData is defined below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 interface Feature { frame : number ; flick? : Readonly < number > ; interpolate? : boolean ; keyframe? : boolean ; bounds ?: [ number , number , number , number ]; // [x1, y1, x2, y2] as (left, top), (bottom, right) geometry? : GeoJSON.FeatureCollection < GeoJSON . Point | GeoJSON . Polygon | GeoJSON . LineString | GeoJSON . Point > ; fishLength? : number ; attributes? : Record < string , unknown > ; head ?: [ number , number ]; tail ?: [ number , number ]; } interface TrackData { trackId : TrackId ; meta : Record < string , unknown > ; attributes : Record < string , unknown > ; confidencePairs : Array < [ string , number ] > ; features : Array < Feature > ; begin : number ; end : number ; } The source TrackData definition can be found here as a TypeScript interface. Example DIVE annotation file This is a relatively simple example, and many optional fields are not included. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"1\" : { \"trackId\" : 1 , \"meta\" : {}, \"attributes\" : {}, \"confidencePairs\" : [[ \"fish\" , 0.87 ], [ \"rock\" , 0.22 ]], \"features\" : [ { \"frame\" : 0 , \"bounds\" : [ 0 , 0 , 10 , 10 ], \"interpolate\" : true }, { \"frame\" : 2 , \"bounds\" : [ 10 , 10 , 20 , 20 ] }, ], \"begin\" : 0 , \"end\" : 2 , }, }","title":"DIVE Annotation JSON"},{"location":"DataFormats/#dive-configuration-json","text":"This information provides the specification for an individual dataset. It consists of the following. Allowed types (or labels) and their appearances are defined by customTypeStyling Preset confidence filters for those types are defined in confidenceFilters Track and Detection attribute specifications are defined in attributes The full DatasetMetaMutable definition can be found here . 1 2 3 4 5 6 interface DatasetMetaMutable { version : number ; customTypeStyling? : Record < string , CustomStyle > ; confidenceFilters? : Record < string , number > ; attributes? : Readonly < Record < string , Attribute >> ; }","title":"DIVE Configuration JSON"},{"location":"DataFormats/#viame-csv","text":"Read the VIAME CSV Specification .","title":"VIAME CSV"},{"location":"DataFormats/#coco-and-kwcoco","text":"Read the COCO Specification Read the KWCOCO Specification","title":"COCO and KWCOCO"},{"location":"DataFormats/#kwiver-packet-format-kpf","text":"Read the KPF Specification","title":"KWIVER Packet Format (KPF)"},{"location":"Deployment-Docker-Compose/","text":"Running with Docker Compose Start here once you have SSH access and sudo privileges for a server or VM. Note Docker server installation is only supported on Linux distributions Container Images A DIVE Web deployment consists of 2 main services. kitware/viame-web - the web server kitware/viame-worker - the queue worker In addition, a database (MongoDB) and a queue service (RabbitMQ) are required. Install dependencies SSH into the target server and install these system dependencies. Tip You can skip this section if you used Ansible to configure your server, as it already installed all necessary dependencies. Install NVIDIA driver version >= 450.80.02 and CUDA 11.0+ sudo ubuntu-drivers install Install docker version 19.03+ guide Install docker-compose version 1.28.0+ guide Install nvidia-container-toolkit Basic deployment Clone this repository and configure options in .env . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Clone this repository git clone https://github.com/Kitware/dive /opt/dive # Change to correct directory cd /opt/dive # Initiate the .env file cp .env.default .env # Edit the .env file # See configuration options below and inline comments nano .env # Pull pre-built images docker-compose pull # Bring the services up # Make sure to specify docker-compose.yml unless you intend to mount code for development docker-compose -f docker-compose.yml up -d VIAME server will be running at http://localhost:8010 . You should see a page that looks like this. The default username and password is admin:letmein . Production deployment If you have a server with a public-facing IP address and a domain name that points to it, you should be able to use our production deployment configuration. This is the way we deploy viame.kitware.com. containrrr/watchtower updates the running containers on a schedule using automated image builds from docker hub (above). linuxserver/duplicati is included to schedule nightly backups, but must be manually configured. You should scale the girder web server up to an appropriate number. This stack will automatically load-balance across however many instances you bring up. 1 2 3 4 5 6 7 8 # Continuing from above, modify .env again to include the production variables nano .env # pull extra containers docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull # scale the web service up docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale girder = 4 Addon management After initial deployment, a DIVE server will only have basic VIAME pipelines available. VIAME optional patches are installed and upgraded using a celery task that must be triggered by hand. Run this by issuing a POST /dive_configuration/upgrade_pipelines request from the swagger UI at http://{domain}/api/v1 . Whether you force or not, only those pipelines from addons from the exact urls passed will be enabled on the server. An old addon can be disabled by simply omitting its download from the upgrade payload. force should be used to force re-download of all URLs in the payload even if their zipfiles have been cached. An upgrade run is always required if the \"common\" pipelines in the base image change. These are updated for every run, and do not require force . See the job log to verify the exact actions taken by an upgrade job. Configuration Reference Web Server config This image contains both the backend and client. Variable Default Description GIRDER_MONGO_URI mongodb://mongo:27017/girder a mongodb connection string GIRDER_ADMIN_USER admin admin username GIRDER_ADMIN_PASS letmein admin password CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string WORKER_API_URL http://girder:8080/api/v1 Address for workers to reach web server There is additional configuration for the RabbitMQ Management plugin. It only matters if you intend to allow individual users to configure private job runners in standalone mode, and can otherwise be ignored. Variable Default Description RABBITMQ_MANAGEMENT_USERNAME guest Management API username RABBITMQ_MANAGEMENT_PASSWORD guest Management API password RABBITMQ_MANAGEMENT_VHOST default Virtual host should match CELERY_BROKER_URL RABBITMQ_MANAGEMENT_URL http://rabbit:15672/ Management API Url You can also pass girder configuration and celery configuration . Worker config This image contains a celery worker to run VIAME pipelines and transcoding jobs. Note : Either a broker url or DIVE credentials must be supplied. Variable Default Description WORKER_WATCHING_QUEUES null one of celery , pipelines , training . Ignored in standalone mode. WORKER_CONCURRENCY # of CPU cores max concurrnet jobs. Lower this if you run training WORKER_GPU_UUID null leave empty to use all GPUs. Specify UUID to use specific device CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string. Ignored in standalone mode. KWIVER_DEFAULT_LOG_LEVEL warn kwiver log level DIVE_USERNAME null Username to start private queue processor. Providing this enables standalone mode. DIVE_PASSWORD null Password for private queue processor. Providing this enables standalone mode. DIVE_API_URL https://viame.kitware.com/api/v1 Remote URL to authenticate against You can also pass regular celery configuration variables . Running the GPU Job Runner in standalone mode Linux Only. Individual users can run a standalone worker to process private jobs from VIAME Web. Install VIAME from the github page to /opt/noaa/viame . Activate the install with source setup_viame.sh . Install VIAME pipeline addons by running cd bin && download_viame_addons.sh from the VIAME install directory. Enable the private user queue for your jobs by visiting the jobs page Run a worker using the docker command below Note : The --volume mount maps to the host installation. You may need to change the source from /opt/noaa/viame depending on your install location, but you should not change the destination from /tmp/addons/extracted . 1 2 3 4 5 6 7 8 9 docker run --rm --name dive_worker \\ --gpus all \\ --ipc host \\ --volume \"/opt/noaa/viame/:/tmp/addons/extracted:ro\" \\ -e \"WORKER_CONCURRENCY=2\" \\ -e \"DIVE_USERNAME=CHANGEME\" \\ -e \"DIVE_PASSWORD=CHANGEME\" \\ -e \"DIVE_API_URL=https://viame.kitware.com/api/v1\" \\ kitware/viame-worker:latest","title":"Running with Docker Compose"},{"location":"Deployment-Docker-Compose/#running-with-docker-compose","text":"Start here once you have SSH access and sudo privileges for a server or VM. Note Docker server installation is only supported on Linux distributions","title":"Running with Docker Compose"},{"location":"Deployment-Docker-Compose/#container-images","text":"A DIVE Web deployment consists of 2 main services. kitware/viame-web - the web server kitware/viame-worker - the queue worker In addition, a database (MongoDB) and a queue service (RabbitMQ) are required.","title":"Container Images"},{"location":"Deployment-Docker-Compose/#install-dependencies","text":"SSH into the target server and install these system dependencies. Tip You can skip this section if you used Ansible to configure your server, as it already installed all necessary dependencies. Install NVIDIA driver version >= 450.80.02 and CUDA 11.0+ sudo ubuntu-drivers install Install docker version 19.03+ guide Install docker-compose version 1.28.0+ guide Install nvidia-container-toolkit","title":"Install dependencies"},{"location":"Deployment-Docker-Compose/#basic-deployment","text":"Clone this repository and configure options in .env . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Clone this repository git clone https://github.com/Kitware/dive /opt/dive # Change to correct directory cd /opt/dive # Initiate the .env file cp .env.default .env # Edit the .env file # See configuration options below and inline comments nano .env # Pull pre-built images docker-compose pull # Bring the services up # Make sure to specify docker-compose.yml unless you intend to mount code for development docker-compose -f docker-compose.yml up -d VIAME server will be running at http://localhost:8010 . You should see a page that looks like this. The default username and password is admin:letmein .","title":"Basic deployment"},{"location":"Deployment-Docker-Compose/#production-deployment","text":"If you have a server with a public-facing IP address and a domain name that points to it, you should be able to use our production deployment configuration. This is the way we deploy viame.kitware.com. containrrr/watchtower updates the running containers on a schedule using automated image builds from docker hub (above). linuxserver/duplicati is included to schedule nightly backups, but must be manually configured. You should scale the girder web server up to an appropriate number. This stack will automatically load-balance across however many instances you bring up. 1 2 3 4 5 6 7 8 # Continuing from above, modify .env again to include the production variables nano .env # pull extra containers docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull # scale the web service up docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale girder = 4","title":"Production deployment"},{"location":"Deployment-Docker-Compose/#addon-management","text":"After initial deployment, a DIVE server will only have basic VIAME pipelines available. VIAME optional patches are installed and upgraded using a celery task that must be triggered by hand. Run this by issuing a POST /dive_configuration/upgrade_pipelines request from the swagger UI at http://{domain}/api/v1 . Whether you force or not, only those pipelines from addons from the exact urls passed will be enabled on the server. An old addon can be disabled by simply omitting its download from the upgrade payload. force should be used to force re-download of all URLs in the payload even if their zipfiles have been cached. An upgrade run is always required if the \"common\" pipelines in the base image change. These are updated for every run, and do not require force . See the job log to verify the exact actions taken by an upgrade job.","title":"Addon management"},{"location":"Deployment-Docker-Compose/#configuration-reference","text":"","title":"Configuration Reference"},{"location":"Deployment-Docker-Compose/#web-server-config","text":"This image contains both the backend and client. Variable Default Description GIRDER_MONGO_URI mongodb://mongo:27017/girder a mongodb connection string GIRDER_ADMIN_USER admin admin username GIRDER_ADMIN_PASS letmein admin password CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string WORKER_API_URL http://girder:8080/api/v1 Address for workers to reach web server There is additional configuration for the RabbitMQ Management plugin. It only matters if you intend to allow individual users to configure private job runners in standalone mode, and can otherwise be ignored. Variable Default Description RABBITMQ_MANAGEMENT_USERNAME guest Management API username RABBITMQ_MANAGEMENT_PASSWORD guest Management API password RABBITMQ_MANAGEMENT_VHOST default Virtual host should match CELERY_BROKER_URL RABBITMQ_MANAGEMENT_URL http://rabbit:15672/ Management API Url You can also pass girder configuration and celery configuration .","title":"Web Server config"},{"location":"Deployment-Docker-Compose/#worker-config","text":"This image contains a celery worker to run VIAME pipelines and transcoding jobs. Note : Either a broker url or DIVE credentials must be supplied. Variable Default Description WORKER_WATCHING_QUEUES null one of celery , pipelines , training . Ignored in standalone mode. WORKER_CONCURRENCY # of CPU cores max concurrnet jobs. Lower this if you run training WORKER_GPU_UUID null leave empty to use all GPUs. Specify UUID to use specific device CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string. Ignored in standalone mode. KWIVER_DEFAULT_LOG_LEVEL warn kwiver log level DIVE_USERNAME null Username to start private queue processor. Providing this enables standalone mode. DIVE_PASSWORD null Password for private queue processor. Providing this enables standalone mode. DIVE_API_URL https://viame.kitware.com/api/v1 Remote URL to authenticate against You can also pass regular celery configuration variables .","title":"Worker config"},{"location":"Deployment-Docker-Compose/#running-the-gpu-job-runner-in-standalone-mode","text":"Linux Only. Individual users can run a standalone worker to process private jobs from VIAME Web. Install VIAME from the github page to /opt/noaa/viame . Activate the install with source setup_viame.sh . Install VIAME pipeline addons by running cd bin && download_viame_addons.sh from the VIAME install directory. Enable the private user queue for your jobs by visiting the jobs page Run a worker using the docker command below Note : The --volume mount maps to the host installation. You may need to change the source from /opt/noaa/viame depending on your install location, but you should not change the destination from /tmp/addons/extracted . 1 2 3 4 5 6 7 8 9 docker run --rm --name dive_worker \\ --gpus all \\ --ipc host \\ --volume \"/opt/noaa/viame/:/tmp/addons/extracted:ro\" \\ -e \"WORKER_CONCURRENCY=2\" \\ -e \"DIVE_USERNAME=CHANGEME\" \\ -e \"DIVE_PASSWORD=CHANGEME\" \\ -e \"DIVE_API_URL=https://viame.kitware.com/api/v1\" \\ kitware/viame-worker:latest","title":"Running the GPU Job Runner in standalone mode"},{"location":"Deployment-Overview/","text":"Deployment Overview The goal of this page is to provide an overview of the ways to run VIAME or VIAME Web in various types of compute environments. Contents Using our deployment of VIAME Web Running your own instance of VIAME Web Using the VIAME command line and project folders in a cloud environment Hybrid options for using local or cloud compute resources with an existing deployment Hybrid options for integrating data from cloud storage such as GCP Buckets or S3 into an existing deployment Our server vs running your own Using our server Running your own Free to use; no maintenance costs You pay hosting and maintenance costs Always up to date Possible to configure automated updates One shared environment for everyone Your organization has full control over access Our team monitors this service for errors and can respond to issues proactively Support requires logs, screenshots, and other error information if applicable Our team can provide guidance on annotation and training because we have direct access to your data Support usually requires example data and annotations Having user data in our environment helps us understand user needs and improve the product Feedback is always appreciated. Limited shared compute resources (2 GPUs) available to process jobs. Can be mitigated by hybrid compute options As much compute as you pay for Using our public server The easiest option to get started using VIAME is to try our public server . Running your own instance You may wish to run your own deployment of VIAME Web in your lab or a cloud environment. Deploying VIAME Web is relatively straightforward with docker-compose . Environment Instructions Local server If you already have SSH access to an existing server and sudo permissions, proceed to the docker compose guide . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 1 AWS / Azure Create a server on your own through the cloud management console, then proceed to the docker compose guide . VIAME CLI with project folders You may not want to use the web annotator and job orchestration at all, and instead run VIAME using the command line in a cloud environment with GPU. Environment Instructions Local server This is a standard VIAME install. See the VIAME documentation install instructions . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 2 AWS / Azure Create a server through the cloud management console, then proceed to the VIAME documentation install instructions . Hybrid options for compute Instead of running the whole web stack, it's possible to deploy a worker by itself to process compute-intensive jobs. This is referred to in the docs as standalone mode . For example, you could: Upload and annotate at viame.kitware.com, but run your own private worker on a lab workstation Deploy your own web server to a local lab workstation, but process your jobs in an ephemeral Google Cloud VM. How it works You must toggle your private queue When you launch jobs (like transcoding, pipelines, or training), they go into a special queue just for your user account. You are responsible for running a worker. Your worker is a Celery process that will connect to our public RabbitMQ server. Jobs submitted through the interface at viame.kitware.com will run on your compute resources. This involves automatically downloading the video or images and annotation files, running a kwiver pipeline, and uploading the results. To set up a private worker, continue to the Provisioning Google Cloud page for Scenario 3 . Hybrid options for storage Any instance of VIAME Web, including our public server, can connect to S3-compatible storage. This means your lab or group could make your existing data available at viame.kitware.com , either privately or publicly. Storage Product Support level Google Cloud Buckets Use as backing storage, import existing data, monitor for changes and automatically discover new uploads AWS S3 Use as backing storage, import existing data MinIO Use as backing storage, import existing data Azure Blob Storage Limited import support using MinIO Azure Gateway Get Help Contact us for support with any of these topics.","title":"Deployment Options Overview"},{"location":"Deployment-Overview/#deployment-overview","text":"The goal of this page is to provide an overview of the ways to run VIAME or VIAME Web in various types of compute environments.","title":"Deployment Overview"},{"location":"Deployment-Overview/#contents","text":"Using our deployment of VIAME Web Running your own instance of VIAME Web Using the VIAME command line and project folders in a cloud environment Hybrid options for using local or cloud compute resources with an existing deployment Hybrid options for integrating data from cloud storage such as GCP Buckets or S3 into an existing deployment","title":"Contents"},{"location":"Deployment-Overview/#our-server-vs-running-your-own","text":"Using our server Running your own Free to use; no maintenance costs You pay hosting and maintenance costs Always up to date Possible to configure automated updates One shared environment for everyone Your organization has full control over access Our team monitors this service for errors and can respond to issues proactively Support requires logs, screenshots, and other error information if applicable Our team can provide guidance on annotation and training because we have direct access to your data Support usually requires example data and annotations Having user data in our environment helps us understand user needs and improve the product Feedback is always appreciated. Limited shared compute resources (2 GPUs) available to process jobs. Can be mitigated by hybrid compute options As much compute as you pay for","title":"Our server vs running your own"},{"location":"Deployment-Overview/#using-our-public-server","text":"The easiest option to get started using VIAME is to try our public server .","title":"Using our public server"},{"location":"Deployment-Overview/#running-your-own-instance","text":"You may wish to run your own deployment of VIAME Web in your lab or a cloud environment. Deploying VIAME Web is relatively straightforward with docker-compose . Environment Instructions Local server If you already have SSH access to an existing server and sudo permissions, proceed to the docker compose guide . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 1 AWS / Azure Create a server on your own through the cloud management console, then proceed to the docker compose guide .","title":"Running your own instance"},{"location":"Deployment-Overview/#viame-cli-with-project-folders","text":"You may not want to use the web annotator and job orchestration at all, and instead run VIAME using the command line in a cloud environment with GPU. Environment Instructions Local server This is a standard VIAME install. See the VIAME documentation install instructions . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 2 AWS / Azure Create a server through the cloud management console, then proceed to the VIAME documentation install instructions .","title":"VIAME CLI with project folders"},{"location":"Deployment-Overview/#hybrid-options-for-compute","text":"Instead of running the whole web stack, it's possible to deploy a worker by itself to process compute-intensive jobs. This is referred to in the docs as standalone mode . For example, you could: Upload and annotate at viame.kitware.com, but run your own private worker on a lab workstation Deploy your own web server to a local lab workstation, but process your jobs in an ephemeral Google Cloud VM. How it works You must toggle your private queue When you launch jobs (like transcoding, pipelines, or training), they go into a special queue just for your user account. You are responsible for running a worker. Your worker is a Celery process that will connect to our public RabbitMQ server. Jobs submitted through the interface at viame.kitware.com will run on your compute resources. This involves automatically downloading the video or images and annotation files, running a kwiver pipeline, and uploading the results. To set up a private worker, continue to the Provisioning Google Cloud page for Scenario 3 .","title":"Hybrid options for compute"},{"location":"Deployment-Overview/#hybrid-options-for-storage","text":"Any instance of VIAME Web, including our public server, can connect to S3-compatible storage. This means your lab or group could make your existing data available at viame.kitware.com , either privately or publicly. Storage Product Support level Google Cloud Buckets Use as backing storage, import existing data, monitor for changes and automatically discover new uploads AWS S3 Use as backing storage, import existing data MinIO Use as backing storage, import existing data Azure Blob Storage Limited import support using MinIO Azure Gateway","title":"Hybrid options for storage"},{"location":"Deployment-Overview/#get-help","text":"Contact us for support with any of these topics.","title":"Get Help"},{"location":"Deployment-Provision/","text":"Cloud Deployment Guide Note Be sure to read the Deployment Overview first. Scenario 1 : Deploy your own instance of VIAME Web to GCP Compute Engine. Scenario 2 : Run VIAME pipelines on a GCP Compute Engine VM from the command line. Scenario 3 : Run a Private GPU worker in GCP to process jobs from any VIAME Web instance including viame.kitware.com (standalone mode) The terraform section is the same for all scenarios. The Ansible section will have differences. Before you begin You'll need a GCP Virtual Machine (VM) with the features listed below. This section will guide you through creating one and deploying VIAME using Terraform and Ansible. Terraform automates the process of creating and destroying cloud resources such as VMs. Ansible automates configuration, such as software installation, on newly created machines. Together, these tools allow you to quickly create a reproducible environment. If you do not want to use these tools, you can create your own VM manually through the management console and skip to the docker documentation instead . Feature Recommended value Operating system Ubuntu 20.04 Instance Type n1-standard-4 or larger GPU Type nvidia-tesla-t4 , nvidia-tesla-p4 , or similar Disk Type SSD, 128GB or more depending on your needs Install dependencies To run the provisioning tools below, you need the following installed on your own workstation. Note Google Cloud worker provisioning can only be done from an Ubuntu Linux 18.04+ host. Ansible and terraform should work on Windows Subsystem for Linux (WSL) if you only have a windows host. You could also use a cheap CPU-only cloud instance to run these tools. Install Google Cloud SDK Install Terraform Install Ansible Find your google cloud project id. It looks like project-name-123456 . Tip Google Cloud imposes GPU Quotas. You may need to request a quota increase . Anecdotally, request increases of 1 unit are approved automatically, but more are rejected. Creating a VM with Terraform 1 2 3 4 5 6 # Clone the dive repo git clone https://github.com/Kitware/dive.git cd dive/devops # Generate a new ssh key ssh-keygen -t ed25519 -f ~/.ssh/gcloud_key Run Terraform Warning GPU-accelerated VMs are significantly more expensive than typical VMs. Make sure you are familiar with the cost of the machine and GPU you choose. See main.tf for default values. See devops/main.tf for a complete list of variables. The default machine_type , gpu_type , and gpu_count can be overridden. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Authenticate with google cloud gcloud auth application-default login # Verify your GPU Quota # https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus # REGION might change. gcloud compute regions describe us-central1 # Run plan, providing any variables you choose terraform plan \\ -var \"project_name=<GCloud-Project-Id>\" \\ -var \"gpu_count=1\" \\ -out create.plan # Run apply. It may take several minutes terraform apply create.plan Destroy the stack Later, when you are done with the server and have backed up your data, use terraform to destroy your resources. 1 terraform destroy -var \"project_name=<GCloud-Project-Id>\" Configure with Ansible This step will prepare the new host to run a VIAME worker by installing nvidia drivers, docker, and downloading VIAME and all optional addons. Warning The playbook may take 30 minutes or more to run because it must install nvidia drivers and download several GB of software packages. Ansible Extra Vars These are all the variables that can be provided with --extra-vars . Variable Default Description run_server no Set run_server=yes for scenario 1 (Web Instance) (Fastest option) run_viame_cli no Set run_viame_cli=yes for scenario 2 (VIAME CLI) run_worker_container no Set run_worker_container=yes for scenario 3 (Standalone Worker) viame_bundle_url latest bundle url Optional for scenario 2 & 3. Change to install a different version of VIAME. This should be a link to the latest Ubuntu Desktop (18/20) binaries from viame.kitware.com (Mirror 1) DIVE_USERNAME null Required for scenario 3. Username to start private queue processor DIVE_PASSWORD null Required for scenario 3. Password for private queue processor WORKER_CONCURRENCY 2 Optional for scenario 3. Max concurrnet jobs. Change this to 1 if you run training DIVE_API_URL https://viame.kitware.com/api/v1 Optional for scenario 3. Remote URL to authenticate against. KWIVER_DEFAULT_LOG_LEVEL warn Optional for scenario 3. kwiver log level Run Ansible The examples below assumes the inventory file was created by Terraform above. 1 2 3 4 5 6 7 8 9 10 11 12 13 # install galaxy plugins ansible-galaxy install -r ansible/requirements.yml # Choose only 1 of the scenarios below # Scenario 1 (Web Instance) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_server=yes\" # Scenario 2 (VIAME CLI) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_viame_cli=yes\" # Scenario 3 (Standalone Worker) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_worker_container=yes DIVE_USERNAME=username DIVE_PASSWORD=changeme\" Once provisioning is complete, jobs should begin processing from the job queue. You can check viame.kitware.com/#/jobs to see queue progress and logs. Tip This Ansible playbook is runnable from any Ubuntu 18.04+ host to any Ubuntu 18.04+ target. To run it locally, use the inventory.local file instead. If you already have nvidia or docker installed, you can comment out these lines in the playbook. If you run locally you'll need to restart the machine and run the playbook a second time. The playbook will do this automatically for remote provisioning, but cannot restart if you're provisioning localhost. 1 ansible-playbook --ask-become-pass -i inventory ansible/playbook.yml --extra-vars \"<see above>\" Tip You may need to run through the docker post-install guide if you have permissions errors when trying to run docker . Check that it worked 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Log in with the ip address from the inventory file (or google cloud dashboard) ssh -i ~/.ssh/gcloud_key viame@ip-address # Test nvidia docker installation docker run --gpus = all --rm nvidia/cuda:11.0-base nvidia-smi # Test regular nvidia runtime nvidia-smi # For Scenario 2 and 3, check KWIVER installation cd /opt/noaa/viame source setup_viame.sh kwiver # For Scenario 3, you can check to see if the worker is started # You should see \"celery@identifier ready.\" in the logs sudo docker logs -f worker You can enable your private queue on the jobs page and begin running jobs. Next Steps Scenario 1 : Proceed to Docker Compose Deployment . Scenario 2 : Setup is complete. Proceed to the VIAME Documentation . Scenario 3 : Setup is complete. Make sure your private queue is enabled. Troubleshooting Ansible provisioning is idempotent. If it fails, run it again once or twice. You may need to change the global GPUS_ALL_REGIONS quota in IAM -> Quotas Nvidia drivers may not install correctly the first time. Try installing manually using ubuntu-drivers","title":"Provisioning Google Cloud"},{"location":"Deployment-Provision/#cloud-deployment-guide","text":"Note Be sure to read the Deployment Overview first. Scenario 1 : Deploy your own instance of VIAME Web to GCP Compute Engine. Scenario 2 : Run VIAME pipelines on a GCP Compute Engine VM from the command line. Scenario 3 : Run a Private GPU worker in GCP to process jobs from any VIAME Web instance including viame.kitware.com (standalone mode) The terraform section is the same for all scenarios. The Ansible section will have differences.","title":"Cloud Deployment Guide"},{"location":"Deployment-Provision/#before-you-begin","text":"You'll need a GCP Virtual Machine (VM) with the features listed below. This section will guide you through creating one and deploying VIAME using Terraform and Ansible. Terraform automates the process of creating and destroying cloud resources such as VMs. Ansible automates configuration, such as software installation, on newly created machines. Together, these tools allow you to quickly create a reproducible environment. If you do not want to use these tools, you can create your own VM manually through the management console and skip to the docker documentation instead . Feature Recommended value Operating system Ubuntu 20.04 Instance Type n1-standard-4 or larger GPU Type nvidia-tesla-t4 , nvidia-tesla-p4 , or similar Disk Type SSD, 128GB or more depending on your needs","title":"Before you begin"},{"location":"Deployment-Provision/#install-dependencies","text":"To run the provisioning tools below, you need the following installed on your own workstation. Note Google Cloud worker provisioning can only be done from an Ubuntu Linux 18.04+ host. Ansible and terraform should work on Windows Subsystem for Linux (WSL) if you only have a windows host. You could also use a cheap CPU-only cloud instance to run these tools. Install Google Cloud SDK Install Terraform Install Ansible Find your google cloud project id. It looks like project-name-123456 . Tip Google Cloud imposes GPU Quotas. You may need to request a quota increase . Anecdotally, request increases of 1 unit are approved automatically, but more are rejected.","title":"Install dependencies"},{"location":"Deployment-Provision/#creating-a-vm-with-terraform","text":"1 2 3 4 5 6 # Clone the dive repo git clone https://github.com/Kitware/dive.git cd dive/devops # Generate a new ssh key ssh-keygen -t ed25519 -f ~/.ssh/gcloud_key","title":"Creating a VM with Terraform"},{"location":"Deployment-Provision/#run-terraform","text":"Warning GPU-accelerated VMs are significantly more expensive than typical VMs. Make sure you are familiar with the cost of the machine and GPU you choose. See main.tf for default values. See devops/main.tf for a complete list of variables. The default machine_type , gpu_type , and gpu_count can be overridden. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Authenticate with google cloud gcloud auth application-default login # Verify your GPU Quota # https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus # REGION might change. gcloud compute regions describe us-central1 # Run plan, providing any variables you choose terraform plan \\ -var \"project_name=<GCloud-Project-Id>\" \\ -var \"gpu_count=1\" \\ -out create.plan # Run apply. It may take several minutes terraform apply create.plan","title":"Run Terraform"},{"location":"Deployment-Provision/#destroy-the-stack","text":"Later, when you are done with the server and have backed up your data, use terraform to destroy your resources. 1 terraform destroy -var \"project_name=<GCloud-Project-Id>\"","title":"Destroy the stack"},{"location":"Deployment-Provision/#configure-with-ansible","text":"This step will prepare the new host to run a VIAME worker by installing nvidia drivers, docker, and downloading VIAME and all optional addons. Warning The playbook may take 30 minutes or more to run because it must install nvidia drivers and download several GB of software packages.","title":"Configure with Ansible"},{"location":"Deployment-Provision/#ansible-extra-vars","text":"These are all the variables that can be provided with --extra-vars . Variable Default Description run_server no Set run_server=yes for scenario 1 (Web Instance) (Fastest option) run_viame_cli no Set run_viame_cli=yes for scenario 2 (VIAME CLI) run_worker_container no Set run_worker_container=yes for scenario 3 (Standalone Worker) viame_bundle_url latest bundle url Optional for scenario 2 & 3. Change to install a different version of VIAME. This should be a link to the latest Ubuntu Desktop (18/20) binaries from viame.kitware.com (Mirror 1) DIVE_USERNAME null Required for scenario 3. Username to start private queue processor DIVE_PASSWORD null Required for scenario 3. Password for private queue processor WORKER_CONCURRENCY 2 Optional for scenario 3. Max concurrnet jobs. Change this to 1 if you run training DIVE_API_URL https://viame.kitware.com/api/v1 Optional for scenario 3. Remote URL to authenticate against. KWIVER_DEFAULT_LOG_LEVEL warn Optional for scenario 3. kwiver log level","title":"Ansible Extra Vars"},{"location":"Deployment-Provision/#run-ansible","text":"The examples below assumes the inventory file was created by Terraform above. 1 2 3 4 5 6 7 8 9 10 11 12 13 # install galaxy plugins ansible-galaxy install -r ansible/requirements.yml # Choose only 1 of the scenarios below # Scenario 1 (Web Instance) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_server=yes\" # Scenario 2 (VIAME CLI) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_viame_cli=yes\" # Scenario 3 (Standalone Worker) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_worker_container=yes DIVE_USERNAME=username DIVE_PASSWORD=changeme\" Once provisioning is complete, jobs should begin processing from the job queue. You can check viame.kitware.com/#/jobs to see queue progress and logs. Tip This Ansible playbook is runnable from any Ubuntu 18.04+ host to any Ubuntu 18.04+ target. To run it locally, use the inventory.local file instead. If you already have nvidia or docker installed, you can comment out these lines in the playbook. If you run locally you'll need to restart the machine and run the playbook a second time. The playbook will do this automatically for remote provisioning, but cannot restart if you're provisioning localhost. 1 ansible-playbook --ask-become-pass -i inventory ansible/playbook.yml --extra-vars \"<see above>\" Tip You may need to run through the docker post-install guide if you have permissions errors when trying to run docker .","title":"Run Ansible"},{"location":"Deployment-Provision/#check-that-it-worked","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Log in with the ip address from the inventory file (or google cloud dashboard) ssh -i ~/.ssh/gcloud_key viame@ip-address # Test nvidia docker installation docker run --gpus = all --rm nvidia/cuda:11.0-base nvidia-smi # Test regular nvidia runtime nvidia-smi # For Scenario 2 and 3, check KWIVER installation cd /opt/noaa/viame source setup_viame.sh kwiver # For Scenario 3, you can check to see if the worker is started # You should see \"celery@identifier ready.\" in the logs sudo docker logs -f worker You can enable your private queue on the jobs page and begin running jobs.","title":"Check that it worked"},{"location":"Deployment-Provision/#next-steps","text":"Scenario 1 : Proceed to Docker Compose Deployment . Scenario 2 : Setup is complete. Proceed to the VIAME Documentation . Scenario 3 : Setup is complete. Make sure your private queue is enabled.","title":"Next Steps"},{"location":"Deployment-Provision/#troubleshooting","text":"Ansible provisioning is idempotent. If it fails, run it again once or twice. You may need to change the global GPUS_ALL_REGIONS quota in IAM -> Quotas Nvidia drivers may not install correctly the first time. Try installing manually using ubuntu-drivers","title":"Troubleshooting"},{"location":"Deployment-Storage/","text":"Cloud Storage Integration This page is intended for storage administrators who would like to make their existing data available through VIAME Web. Tip This guide assumes you are working with viame.kitware.com . If you are using a different deployment, be sure to change the appropriate fields. Tip Regarding data transfer costs, if you choose to keep both your data storage and job runners in Google Cloud (or AWS), you will avoid paying a data egress fee for transferring data between storage and the processing node. Google Cloud Storage Mirroring DIVE Web can mirror your data from Google Cloud storage buckets such that your team fully controls upload and data organization, but is able to view, annotate, and run analysis within VIAME Web. Creating access credentials Create a new service account with read-only access to the bucket(s) and prefixes that you want to map. In storage settings , in the interoperability tab, create an access key (Service account HMAC) for your read-only service account. Setting up CORS You'll also need to configure CORS headers for any buckets where media will be served. Save the following snippet as bucket-cors-config.json . 1 2 3 4 5 6 7 8 [ { \"origin\" : [ \"https://viame.kitware.com\" ], \"method\" : [ \"GET\" ], \"responseHeader\" : [ \"Content-Type\" ], \"maxAgeSeconds\" : 3600 } ] Then use gsutils to configure each bucket. 1 gsutil cors set bucket-cors-config.json gs://BUCKET_NAME Pub/Sub notifications To keep the mount up-to-date with new data added to your bucket, please create a Pub/Sub subscription on the bucket. Create a bucket notification configuration Create a topic subscription Set a push delivery method for the subsciption The URL for delivery should be https://viame.kitware.com/api/v1/bucket_notifications/gcs Our server will process events from this subscription to keep your data current. Choose a mount point Choose a folder as a mount-point inside DIVE Web. This folder should ideally be dedicated to mapping from your GCS buckets. We recommend creating a Google Cloud Storage folder with subfolders named for each bucket in your user's workspace. You can do this using the New Folder button in DIVE Web's File Browser. You can get the folder ID from the browser's URL bar. Send us the details Send an email with the following details from above to viame-web@kitware.com . 1 2 3 4 5 6 7 8 subject: Add a google cloud storage bucket mount Bucket name: Service provider: Google cloud Access Key: Secret Key: Mount point folder: Prefix (if applicable): S3 and MinIO Mirroring If you have data in S3 or MinIO, you can mirror it in DIVE for annotation. Data is expected to be either videos or images organized into folders You should not make changes to folder contents once a folder has been mirrored into DIVE. Adding or removing images in a particular folder may cause annotation alignment issues. Adding entire new folders is supported, and will require a re-index of your S3 bucket. Mirroring setup To create a bucket mirror on your own DIVE deployment. Open /girder#assetstores and create a new S3 assetstore. Mark as Read only . Choose the green \"Begin Import\" button on the new assetstore. Choose a prefix within your bucket to mirror (probably just / ) and a destination folder ID. You can get the destination folder ID from the URL of the folder location in your address bar.","title":"Cloud Storage Integration"},{"location":"Deployment-Storage/#cloud-storage-integration","text":"This page is intended for storage administrators who would like to make their existing data available through VIAME Web. Tip This guide assumes you are working with viame.kitware.com . If you are using a different deployment, be sure to change the appropriate fields. Tip Regarding data transfer costs, if you choose to keep both your data storage and job runners in Google Cloud (or AWS), you will avoid paying a data egress fee for transferring data between storage and the processing node.","title":"Cloud Storage Integration"},{"location":"Deployment-Storage/#google-cloud-storage-mirroring","text":"DIVE Web can mirror your data from Google Cloud storage buckets such that your team fully controls upload and data organization, but is able to view, annotate, and run analysis within VIAME Web.","title":"Google Cloud Storage Mirroring"},{"location":"Deployment-Storage/#creating-access-credentials","text":"Create a new service account with read-only access to the bucket(s) and prefixes that you want to map. In storage settings , in the interoperability tab, create an access key (Service account HMAC) for your read-only service account.","title":"Creating access credentials"},{"location":"Deployment-Storage/#setting-up-cors","text":"You'll also need to configure CORS headers for any buckets where media will be served. Save the following snippet as bucket-cors-config.json . 1 2 3 4 5 6 7 8 [ { \"origin\" : [ \"https://viame.kitware.com\" ], \"method\" : [ \"GET\" ], \"responseHeader\" : [ \"Content-Type\" ], \"maxAgeSeconds\" : 3600 } ] Then use gsutils to configure each bucket. 1 gsutil cors set bucket-cors-config.json gs://BUCKET_NAME","title":"Setting up CORS"},{"location":"Deployment-Storage/#pubsub-notifications","text":"To keep the mount up-to-date with new data added to your bucket, please create a Pub/Sub subscription on the bucket. Create a bucket notification configuration Create a topic subscription Set a push delivery method for the subsciption The URL for delivery should be https://viame.kitware.com/api/v1/bucket_notifications/gcs Our server will process events from this subscription to keep your data current.","title":"Pub/Sub notifications"},{"location":"Deployment-Storage/#choose-a-mount-point","text":"Choose a folder as a mount-point inside DIVE Web. This folder should ideally be dedicated to mapping from your GCS buckets. We recommend creating a Google Cloud Storage folder with subfolders named for each bucket in your user's workspace. You can do this using the New Folder button in DIVE Web's File Browser. You can get the folder ID from the browser's URL bar.","title":"Choose a mount point"},{"location":"Deployment-Storage/#send-us-the-details","text":"Send an email with the following details from above to viame-web@kitware.com . 1 2 3 4 5 6 7 8 subject: Add a google cloud storage bucket mount Bucket name: Service provider: Google cloud Access Key: Secret Key: Mount point folder: Prefix (if applicable):","title":"Send us the details"},{"location":"Deployment-Storage/#s3-and-minio-mirroring","text":"If you have data in S3 or MinIO, you can mirror it in DIVE for annotation. Data is expected to be either videos or images organized into folders You should not make changes to folder contents once a folder has been mirrored into DIVE. Adding or removing images in a particular folder may cause annotation alignment issues. Adding entire new folders is supported, and will require a re-index of your S3 bucket.","title":"S3 and MinIO Mirroring"},{"location":"Deployment-Storage/#mirroring-setup","text":"To create a bucket mirror on your own DIVE deployment. Open /girder#assetstores and create a new S3 assetstore. Mark as Read only . Choose the green \"Begin Import\" button on the new assetstore. Choose a prefix within your bucket to mirror (probably just / ) and a destination folder ID. You can get the destination folder ID from the URL of the folder location in your address bar.","title":"Mirroring setup"},{"location":"Dive-Desktop/","text":"DIVE Desktop DIVE is available as an electron based desktop application with VIAME integration. It has most of the same UI and features web. You may want to use desktop if... You want to make use of GPUs on your own workstation You need to use DIVE without network access You have large quantities of data on disk impractical for uploading to a server. DIVE Desktop is fully supported on Windows and Linux. MacOS users can use it as an annotator, but without NVIDIA Driver support, the machine learning features from VIAME are unavailable. Installation Download the latest DIVE Desktop from GitHub Choose an asset from the list matching your operating system: OS Extension Description Windows .exe Portable executable (recommended) Windows .msi Installer file MacOS .dmg MacOS DiskImage (Intel only, M1 not supported) Linux .AppImage Portable executable for all Linux platforms (recommended) Linux .snap Ubuntu SnapCraft package Full VIAME Desktop Installation This is the installation guide for DIVE. If you want the full VIAME toolkit, you can get it from github.com/viame/viame . The full toolkit installation includes DIVE. Supported Dataset Types DIVE Desktop supports single- and multi-camera datasets. Single Camera Dataset is the most common option. Single camera datasets are supported by the majority of VIAME pipeline and training configurations. Stereo Datasets are for datasets collected from a camera rig with a left and right camera. These datasets can be used with certain specialty VIAME pipelines. Their physical relationship may be described by a camera transform .npz file (numpy transformation matrix). Multi-Cam Datasets are for more generic multi-camera rig setups. They may have overlapping fields of view. Importing Datasets Click either Open Image Sequence or Open Video to begin a single camera default import. Click the dropdown button to show additional import options. From File is the default option for videos. It will open a file picker and allow you to choose a single video file. Directory is the default option for image sequences. It will prompt you to choose an entire folder of images to import as a dataset. You can use globbing patterns to filter the contents of an image directory during import. Click Show advanced options to reveal the glob input. Image List will prompt you to choose a .txt file that contains an image name or full path on each line. Stereo will prompt you to choose 2 videos or 2 image sequences. Multi-Cam will prompt you to describe the multi-cam configuration by naming several cameras and picking the source media for each. The import routine will look for .csv and .json files in the same directory as the source media, and you will be prompted to manually select an annotation file and a configuration file. Neither is required. Video Transcoding DIVE Desktop is an Electron application built on web technologies. Certain video codecs require automatic transcoding to be usable. Video will be transcoded unless all the following conditions are met. codec = h264 sample_aspect_ratio (SAR) = 1:1 Running Training Click on Training to open the training tab. Add one or more datasets to the staging area by clicking . Choose an appropriate training config file and any training parameters. These are documented on the training configuration page . Click Train on (N) Datasets . Note that depending on what configuration and datasets you chose, training could take hours or days. Desktop Settings DIVE Desktop requires a local installation of the VIAME toolkit to run pipelines, train, and do transcoding. VIAME Install Path is set automatically if you use the launch_dive_interface.[bat|sh] script from a VIAME install. Otherwise, you may need to change this yourself. Use Choose to choose the base installation path, then click Save . Project Data Storage Path defaults to a subfolder in your user workspace and should generally not be changed. Read only mode disables the ability to save when using the annotator. Synchronize Recents - The Synchronize Recents with Project Data button is useful if data in the Project Data Storage Path gets out of sync with what appears in the Recents list. Data Storage Path The data storage path is not at all related to \"project folders\" in VIAME. It's just a place for DIVE Desktop to keep and structure all the data it needs to run. A typical data storage directory has 3 subfolders: DIVE_Jobs - Each job run has a working directory, kept here. DIVE_Projects - Each dataset you import into desktop has metadata and annotation data (with revision history) kept here. DIVE_Pipelines - Training runs produce models that get copied into here. Here's an example of structure you might find in the storage path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 VIAME_DATA \u251c\u2500\u2500 DIVE_Jobs \u2502 \u251c\u2500\u2500 Scallop_1_scallop and flatfish_06-01-2021_11-02-11.585 \u2502 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2502 \u2514\u2500\u2500 runlog.txt \u2502 \u2514\u2500\u2500 Scallop_2_scallop netharn_06-01-2021_11-02-19.432 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2514\u2500\u2500 runlog.txt \u251c\u2500\u2500 DIVE_Pipelines \u2502 \u251c\u2500\u2500 My Fish SVM Demo \u2502 \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2502 \u2514\u2500\u2500 fish.svm \u2502 \u2514\u2500\u2500 Quadcam_Fish_Detector_SVM \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2514\u2500\u2500 fish.svm \u2514\u2500\u2500 DIVE_Projects \u251c\u2500\u2500 fish_training_data_c_jp7hq88vfv \u2502 \u251c\u2500\u2500 auxiliary \u2502 \u2502 \u2514\u2500\u2500 result_06-01-2021_10-55-38.627.json \u2502 \u251c\u2500\u2500 meta.json \u2502 \u2514\u2500\u2500 result_06-01-2021_04-53-38.050.json \u2514\u2500\u2500 scallop_2_jrgdq760gu \u251c\u2500\u2500 auxiliary \u2502 \u2514\u2500\u2500 result_06-01-2021_10-54-56.034.json \u251c\u2500\u2500 meta.json \u2514\u2500\u2500 result_06-01-2021_11-02-35.857.json Configuration with env DIVE Desktop looks for the these environment variables on launch. Name Default Description DIVE_VIAME_INSTALL_PATH /opt/noaa/viame (Linux/macOS) C:\\Program Files\\VIAME (Windows) Overrides the location of the VIAME installation. Users may not change this value in the settings pane if provided. DIVE_READONLY_MODE None Overrides read only mode to true or false. Users may still change this value in the settings pane if provided. Import/Export of Models Trained models are kept in ${Project Data Storage Path}/DIVE_Pipelines as described above. Each model file consists of exactly 1 pipe file and some number of other model files. The pipe file can be one of detector.pipe , tracker.pipe , or generate.pipe . Other files can be .zip , .svm , .lbl , or .cfg . You can use externally trained models in DIVE by creating a folder containing these files. The name of the configuration or pipeline in dive will be the folder name you create. Troubleshooting I imported some data, but I don't see my annotations See Importing images and video above . I get an error that says \"ffmpeg not installed, please download and install VIAME Toolkit from the main page\" DIVE Desktop relies on an installation of ffmpeg for transcoding videos and some images. This tool comes with the VIAME installation. Verify your VIAME Install Base Path is correct. Some VIAME canned pipelines are missing, or there are no training configuration files. You may need to install VIAME Toolkit, or correct your VIAME Install Base Path setting. If you don't see some pipelines you expect, you may not have installed the addons (also called Optional Patches) yet. Download and install these based on the VIAME installation docs . Advanced troubleshooting If you experience problems or have questions about DIVE Desktop, contact us and include the content from the settings page such as Build Version as well as your currently installed VIAME version. It's also helpful to look in the debug console. Press Ctrl + Shift + I to launch the Dev Tools and look under the console tab. Errors and warnings will appear in red and yellow. You can right-click in the console area and click \"Save As\" to save the log file and open a support ticket","title":"Desktop Version"},{"location":"Dive-Desktop/#dive-desktop","text":"DIVE is available as an electron based desktop application with VIAME integration. It has most of the same UI and features web. You may want to use desktop if... You want to make use of GPUs on your own workstation You need to use DIVE without network access You have large quantities of data on disk impractical for uploading to a server. DIVE Desktop is fully supported on Windows and Linux. MacOS users can use it as an annotator, but without NVIDIA Driver support, the machine learning features from VIAME are unavailable.","title":"DIVE Desktop"},{"location":"Dive-Desktop/#installation","text":"Download the latest DIVE Desktop from GitHub Choose an asset from the list matching your operating system: OS Extension Description Windows .exe Portable executable (recommended) Windows .msi Installer file MacOS .dmg MacOS DiskImage (Intel only, M1 not supported) Linux .AppImage Portable executable for all Linux platforms (recommended) Linux .snap Ubuntu SnapCraft package","title":"Installation"},{"location":"Dive-Desktop/#full-viame-desktop-installation","text":"This is the installation guide for DIVE. If you want the full VIAME toolkit, you can get it from github.com/viame/viame . The full toolkit installation includes DIVE.","title":"Full VIAME Desktop Installation"},{"location":"Dive-Desktop/#supported-dataset-types","text":"DIVE Desktop supports single- and multi-camera datasets. Single Camera Dataset is the most common option. Single camera datasets are supported by the majority of VIAME pipeline and training configurations. Stereo Datasets are for datasets collected from a camera rig with a left and right camera. These datasets can be used with certain specialty VIAME pipelines. Their physical relationship may be described by a camera transform .npz file (numpy transformation matrix). Multi-Cam Datasets are for more generic multi-camera rig setups. They may have overlapping fields of view.","title":"Supported Dataset Types"},{"location":"Dive-Desktop/#importing-datasets","text":"Click either Open Image Sequence or Open Video to begin a single camera default import. Click the dropdown button to show additional import options. From File is the default option for videos. It will open a file picker and allow you to choose a single video file. Directory is the default option for image sequences. It will prompt you to choose an entire folder of images to import as a dataset. You can use globbing patterns to filter the contents of an image directory during import. Click Show advanced options to reveal the glob input. Image List will prompt you to choose a .txt file that contains an image name or full path on each line. Stereo will prompt you to choose 2 videos or 2 image sequences. Multi-Cam will prompt you to describe the multi-cam configuration by naming several cameras and picking the source media for each. The import routine will look for .csv and .json files in the same directory as the source media, and you will be prompted to manually select an annotation file and a configuration file. Neither is required.","title":"Importing Datasets"},{"location":"Dive-Desktop/#video-transcoding","text":"DIVE Desktop is an Electron application built on web technologies. Certain video codecs require automatic transcoding to be usable. Video will be transcoded unless all the following conditions are met. codec = h264 sample_aspect_ratio (SAR) = 1:1","title":"Video Transcoding"},{"location":"Dive-Desktop/#running-training","text":"Click on Training to open the training tab. Add one or more datasets to the staging area by clicking . Choose an appropriate training config file and any training parameters. These are documented on the training configuration page . Click Train on (N) Datasets . Note that depending on what configuration and datasets you chose, training could take hours or days.","title":"Running Training"},{"location":"Dive-Desktop/#desktop-settings","text":"DIVE Desktop requires a local installation of the VIAME toolkit to run pipelines, train, and do transcoding. VIAME Install Path is set automatically if you use the launch_dive_interface.[bat|sh] script from a VIAME install. Otherwise, you may need to change this yourself. Use Choose to choose the base installation path, then click Save . Project Data Storage Path defaults to a subfolder in your user workspace and should generally not be changed. Read only mode disables the ability to save when using the annotator. Synchronize Recents - The Synchronize Recents with Project Data button is useful if data in the Project Data Storage Path gets out of sync with what appears in the Recents list.","title":"Desktop Settings"},{"location":"Dive-Desktop/#data-storage-path","text":"The data storage path is not at all related to \"project folders\" in VIAME. It's just a place for DIVE Desktop to keep and structure all the data it needs to run. A typical data storage directory has 3 subfolders: DIVE_Jobs - Each job run has a working directory, kept here. DIVE_Projects - Each dataset you import into desktop has metadata and annotation data (with revision history) kept here. DIVE_Pipelines - Training runs produce models that get copied into here. Here's an example of structure you might find in the storage path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 VIAME_DATA \u251c\u2500\u2500 DIVE_Jobs \u2502 \u251c\u2500\u2500 Scallop_1_scallop and flatfish_06-01-2021_11-02-11.585 \u2502 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2502 \u2514\u2500\u2500 runlog.txt \u2502 \u2514\u2500\u2500 Scallop_2_scallop netharn_06-01-2021_11-02-19.432 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2514\u2500\u2500 runlog.txt \u251c\u2500\u2500 DIVE_Pipelines \u2502 \u251c\u2500\u2500 My Fish SVM Demo \u2502 \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2502 \u2514\u2500\u2500 fish.svm \u2502 \u2514\u2500\u2500 Quadcam_Fish_Detector_SVM \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2514\u2500\u2500 fish.svm \u2514\u2500\u2500 DIVE_Projects \u251c\u2500\u2500 fish_training_data_c_jp7hq88vfv \u2502 \u251c\u2500\u2500 auxiliary \u2502 \u2502 \u2514\u2500\u2500 result_06-01-2021_10-55-38.627.json \u2502 \u251c\u2500\u2500 meta.json \u2502 \u2514\u2500\u2500 result_06-01-2021_04-53-38.050.json \u2514\u2500\u2500 scallop_2_jrgdq760gu \u251c\u2500\u2500 auxiliary \u2502 \u2514\u2500\u2500 result_06-01-2021_10-54-56.034.json \u251c\u2500\u2500 meta.json \u2514\u2500\u2500 result_06-01-2021_11-02-35.857.json","title":"Data Storage Path"},{"location":"Dive-Desktop/#configuration-with-env","text":"DIVE Desktop looks for the these environment variables on launch. Name Default Description DIVE_VIAME_INSTALL_PATH /opt/noaa/viame (Linux/macOS) C:\\Program Files\\VIAME (Windows) Overrides the location of the VIAME installation. Users may not change this value in the settings pane if provided. DIVE_READONLY_MODE None Overrides read only mode to true or false. Users may still change this value in the settings pane if provided.","title":"Configuration with env"},{"location":"Dive-Desktop/#importexport-of-models","text":"Trained models are kept in ${Project Data Storage Path}/DIVE_Pipelines as described above. Each model file consists of exactly 1 pipe file and some number of other model files. The pipe file can be one of detector.pipe , tracker.pipe , or generate.pipe . Other files can be .zip , .svm , .lbl , or .cfg . You can use externally trained models in DIVE by creating a folder containing these files. The name of the configuration or pipeline in dive will be the folder name you create.","title":"Import/Export of Models"},{"location":"Dive-Desktop/#troubleshooting","text":"I imported some data, but I don't see my annotations See Importing images and video above . I get an error that says \"ffmpeg not installed, please download and install VIAME Toolkit from the main page\" DIVE Desktop relies on an installation of ffmpeg for transcoding videos and some images. This tool comes with the VIAME installation. Verify your VIAME Install Base Path is correct. Some VIAME canned pipelines are missing, or there are no training configuration files. You may need to install VIAME Toolkit, or correct your VIAME Install Base Path setting. If you don't see some pipelines you expect, you may not have installed the addons (also called Optional Patches) yet. Download and install these based on the VIAME installation docs . Advanced troubleshooting If you experience problems or have questions about DIVE Desktop, contact us and include the content from the settings page such as Build Version as well as your currently installed VIAME version. It's also helpful to look in the debug console. Press Ctrl + Shift + I to launch the Dev Tools and look under the console tab. Errors and warnings will appear in red and yellow. You can right-click in the console area and click \"Save As\" to save the log file and open a support ticket","title":"Troubleshooting"},{"location":"FAQ/","text":"Frequently Asked Questions How do I find existing data to use? The Training Data Collection is organized roughly by domain and collection method. How do I share data with others? This use case is covered on the sharing page . If you want to publish your data so that other groups can use it, please contact us . How do I run analysis workflows on my data? In DIVE, these are called pipelines. You'll need to see what sorts of analysis workflows are currently available on the pipeline page . These sorts of AI workflows are the final goal for most users. They allow the user to quickly perform quantitative analysis to answer questions like how many individuals of each type appear on each image or video frame? If no suitable existing analysis exists for your use case or you aren't sure how to proceed, you're welcome to contact our team and ask for help . How do I create new models? You want to perform analysis (detection, tracking, measurement, etc) on object types not yet covered by the community data and pre-trained analysis pipelines available. This will involve training new models based on ground-truth annotations. Training configurations are listed on the pipeline page . How can I load data incrementally? If you have data in lots of places or it arrives at different times, it's probably best to break these batches or groups into individual datasets and annotate each individually. Using the checkboxes in web, you can use multiple datasets to generate a trained model. Breaking large amounts of data up into manageable groups is generally a good idea. Do users need to transcode their own data? No. VIAME Web and DIVE Desktop perform automatic transcoding if it is necessary. How does video frame alignment work? When you annotate a video in DIVE, the true video is played in the browser using a native HTML5 video player. Web browsers report and control time in floating point seconds rather than using frame numbers, but annotations are created using frame numbers as their time indicators, so it's important to make sure these line up. Most of the time, videos are downsampled for annotation, meaning that the true video framerate (30hz, for example) is annotated at a lower rate, such as 5hz or 10hz. Kwiver (the computer vision tool behind VIAME) uses a downsampling approach that sorts actual frames into downsampled buckets based on the start time of the frame. An implementation of this approach is described here. 1 2 3 4 5 6 7 8 9 10 def get_frame_from_timestamp ( timestamp , true_fps , downsample_fps ): downsampled_frame = timestamp * downsample_fps real_frame = if downsample_fps >= true_fps : # This is a true downsample next_true_frame_boundary = Math . ceil ( timestamp * true_fps ) return Math . floor ( next_true_frame_boundary / downsample_fps ) raise Exception ( 'Real video framerate must be GTE downsample rate' ) There are caveats with this approach. It does not handle padding properly. If a video begins or ends with padding, you may see a black screen in DIVE, but kwiver will wait for the first true frame to use as the representative for the bucket. It does not handle variable width frames properly. If a video has variable width frames, the assumptions about the locations of true frame boundaries do not hold and kwiver training may have alignment issues.","title":"Frequently Asked Questions"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"FAQ/#how-do-i-find-existing-data-to-use","text":"The Training Data Collection is organized roughly by domain and collection method.","title":"How do I find existing data to use?"},{"location":"FAQ/#how-do-i-share-data-with-others","text":"This use case is covered on the sharing page . If you want to publish your data so that other groups can use it, please contact us .","title":"How do I share data with others?"},{"location":"FAQ/#how-do-i-run-analysis-workflows-on-my-data","text":"In DIVE, these are called pipelines. You'll need to see what sorts of analysis workflows are currently available on the pipeline page . These sorts of AI workflows are the final goal for most users. They allow the user to quickly perform quantitative analysis to answer questions like how many individuals of each type appear on each image or video frame? If no suitable existing analysis exists for your use case or you aren't sure how to proceed, you're welcome to contact our team and ask for help .","title":"How do I run analysis workflows on my data?"},{"location":"FAQ/#how-do-i-create-new-models","text":"You want to perform analysis (detection, tracking, measurement, etc) on object types not yet covered by the community data and pre-trained analysis pipelines available. This will involve training new models based on ground-truth annotations. Training configurations are listed on the pipeline page .","title":"How do I create new models?"},{"location":"FAQ/#how-can-i-load-data-incrementally","text":"If you have data in lots of places or it arrives at different times, it's probably best to break these batches or groups into individual datasets and annotate each individually. Using the checkboxes in web, you can use multiple datasets to generate a trained model. Breaking large amounts of data up into manageable groups is generally a good idea.","title":"How can I load data incrementally?"},{"location":"FAQ/#do-users-need-to-transcode-their-own-data","text":"No. VIAME Web and DIVE Desktop perform automatic transcoding if it is necessary.","title":"Do users need to transcode their own data?"},{"location":"FAQ/#how-does-video-frame-alignment-work","text":"When you annotate a video in DIVE, the true video is played in the browser using a native HTML5 video player. Web browsers report and control time in floating point seconds rather than using frame numbers, but annotations are created using frame numbers as their time indicators, so it's important to make sure these line up. Most of the time, videos are downsampled for annotation, meaning that the true video framerate (30hz, for example) is annotated at a lower rate, such as 5hz or 10hz. Kwiver (the computer vision tool behind VIAME) uses a downsampling approach that sorts actual frames into downsampled buckets based on the start time of the frame. An implementation of this approach is described here. 1 2 3 4 5 6 7 8 9 10 def get_frame_from_timestamp ( timestamp , true_fps , downsample_fps ): downsampled_frame = timestamp * downsample_fps real_frame = if downsample_fps >= true_fps : # This is a true downsample next_true_frame_boundary = Math . ceil ( timestamp * true_fps ) return Math . floor ( next_true_frame_boundary / downsample_fps ) raise Exception ( 'Real video framerate must be GTE downsample rate' ) There are caveats with this approach. It does not handle padding properly. If a video begins or ends with padding, you may see a black screen in DIVE, but kwiver will wait for the first true frame to use as the representative for the bucket. It does not handle variable width frames properly. If a video has variable width frames, the assumptions about the locations of true frame boundaries do not hold and kwiver training may have alignment issues.","title":"How does video frame alignment work?"},{"location":"Mouse-Keyboard-Shortcuts/","text":"Mouse and Keyboard Shortcuts General Interactions control description Left Click select track/detection Right Click toggle edit mode Middle Click pan camera Scroll Wheel zoom Mouse Drag pan Shift + mouse drag select area to zoom Up select previous track in list Down select next track in list Esc unselect, exit edit mode A toggle attribute / merge pane Playback control description Left or D previous frame Right or F next frame Space play/pause Editing Most editing controls are available when a track or detection is selected. control description Del delete entire track or detection N create new track or detection Home go to first frame of selected track End go to the last frame of selected track 1 Enter bounding-box edit mode on selection 2 Enter polygon edit mode on selection 3 Enter head/tail/line edit mode on selection H or G while in line mode, place head point next T or Y while in line mode, place tail point next Esc unselect, exit edit mode, exit merge mode K toggle keyframe for the current frame and selected track I toggle interpolation for the current range of the selected track M enter merge mode for on selection Shift + M commit (finalize) merge for selected tracks. Shift + Enter focus class select/text box on selected track in track list. Press Down to open all options. Pres Enter twice to accept an option. Press Esc to unfocus. Adding new shortcuts If you think a new shortcut would be useful, please send us feedback .","title":"Keyboard Shortcut Reference"},{"location":"Mouse-Keyboard-Shortcuts/#mouse-and-keyboard-shortcuts","text":"","title":"Mouse and Keyboard Shortcuts"},{"location":"Mouse-Keyboard-Shortcuts/#general-interactions","text":"control description Left Click select track/detection Right Click toggle edit mode Middle Click pan camera Scroll Wheel zoom Mouse Drag pan Shift + mouse drag select area to zoom Up select previous track in list Down select next track in list Esc unselect, exit edit mode A toggle attribute / merge pane","title":"General Interactions"},{"location":"Mouse-Keyboard-Shortcuts/#playback","text":"control description Left or D previous frame Right or F next frame Space play/pause","title":"Playback"},{"location":"Mouse-Keyboard-Shortcuts/#editing","text":"Most editing controls are available when a track or detection is selected. control description Del delete entire track or detection N create new track or detection Home go to first frame of selected track End go to the last frame of selected track 1 Enter bounding-box edit mode on selection 2 Enter polygon edit mode on selection 3 Enter head/tail/line edit mode on selection H or G while in line mode, place head point next T or Y while in line mode, place tail point next Esc unselect, exit edit mode, exit merge mode K toggle keyframe for the current frame and selected track I toggle interpolation for the current range of the selected track M enter merge mode for on selection Shift + M commit (finalize) merge for selected tracks. Shift + Enter focus class select/text box on selected track in track list. Press Down to open all options. Pres Enter twice to accept an option. Press Esc to unfocus.","title":"Editing"},{"location":"Mouse-Keyboard-Shortcuts/#adding-new-shortcuts","text":"If you think a new shortcut would be useful, please send us feedback .","title":"Adding new shortcuts"},{"location":"Pipeline-Documentation/","text":"Pipelines and Training Both web and desktop versions are capable of running canned pipelines and model training on your ground truth data. This document is to help you decide which pipeline to run. Help me choose Contact our team if you need help choosing the right data analysis strategy. Please upload some sample data to viame.kitware.com to allow us to better assist you. Detection Best for a series of images that have no temporal relationship, such as arial photography of multiple scenes. Also preferred if you only care about aggregate data for the dataset, such as max occurrences of an object per scene. Pipeline Use case arctic seal eo yolo detector for color imagery arctic seal ir yolo detector for infrared em tuna detector for identifying individual features of tuna fish without motion simple single-class fish detector generic proposals generic object detector motion detect regions of motion in video or time-series images pengcam kw Penguin cam full-frame classifier pengcam swfsc Penguin cam full-frame classifier scallop and flatfish detector for benthic images scallop and flatfish left detector for benthic images, process left half of each frame only scallop netharn deep learning detector for benthic images scallop netharn left deep learning detector for benthic images, process left half of each frame only sea lion multi class detects bulls, cows, pups, etc sea lion single class detector sefsc bw group black-and-white fish detector (18 class, lower granularity) (oldest, v1) sefsc bw species v2 black-and-white fish species detector (updated, v2) Tracking Run full tracking pipelines on your data. Appropriate for videos and image sequences that derive from a video. Tracking involves first running a detection pipeline then performing detection linking to form connected object tracks. Note some trackers can perform differently on time-series data depending on the annotation framerate selected when you upload or import your dataset. Higher framerates take longer to process, but may produce better results. Pipeline Use case em tuna tracker fish simple fish tracker fish.sfd tracker generic generic object tracker puts generic boxes around arbitrary objects motion identifies moving object tracks mouss tracker, trained with data from MOUSS (Modular Optical Underwater Survey System) sefsc bw * same as above, but with tracking Utility An assortment of other types of utility pipelines. Utility pipelines are named utility_<name>.pipe and are unique in that they may take detections as inputs (but are not required to). Pipeline Use case add segmentations watershed Transform existing bounding boxes into polygons empty frame lbls {N}fr Add an empty bounding box covering the whole media element for the purpose of adding full-frame classifier attributes. Unique tracks are created every N frames. track user selections Create tracks from user-initialized detection bounding boxes. Draw a box on the first frame of a track, and the pipeline will continue tracking the selected object(s) Training Run model training on ground truth annotations. Currently, training configurations are available to do object detection, object classification, and full-frame classification. Tracker training will be added in a future update. Full-frame classifiers can be trained on arbitrary multi-class labels. It's helpful to start with empty frame lbls utility pipe and add type annotations to each generated frame. Object classifiers and detectors are trained on bounding boxes with arbitrary multi-class labels. Overview SVM ( Support Vector Machine ) configurations are usable with the smallest amount of ground-truth and train relatively quickly. NetHarn is a pytorch deep learning framework that requires more input data: on the order of thousands of target examples. There are two architectures used. Netharn models can take up to several days to train. Cascade Faster R-CNN (cfrnn) for training box detectors Mask R-CNN for training pixel classification and box detection ResNet (Residual Network) for training full frame or secondary object classifiers Options New Model Name A recognizable name for the pipeline that results from the training run. Configuration File One of the configuration options in the table below. Labels.txt file This optional file controls the output classes that a newly trained model will generate. Use if you annotated using higher granularity labels (such as species names) and want to train a classifier using more Or you want to restrict your training session to only train on certain kinds of ground-truth data. The following example labels.txt shows how to train a FISH classifier by combining redfish and bluefish , preserve the ROCK label, and omit every other label. 1 2 FISH redfish bluefish ROCK By default, all classes from all input datasets are preserved in the output model. Use annotation frames only By default, training runs include all frames from the chosen input datasets, and frames without annotations are considered negative examples. If you choose to use annotated frames only, frames or images with zero annotations will be discarded. This option is useful for trying to train on datasets that are only partially annotated. Configurations Configuration Availability Use Case detector_default both alias: train detector netharn cfrnn detector_netharn_cfrnn both detector_netharn_mask_rcnn both detector_svm_over_generic_detections both general purpose svm detector_svm_over_fish_detections both fish svm frame_classifier_default both alias: frame classifier netharn resnet frame_classifier_netharn_resnet both full-frame frame_classifier_svm_overn_resnet both full-frame object_classifier_default both alias: netharn resnet object classifier object_classifier_netharn_resnet both yolo desktop only can train, but resulting model is not runnable with desktop yet Pipeline Import and Export Pipelines created outside of VIAME Web can be upload and shared with other users. See Pipeline Import and Export for details.","title":"Pipelines & Training"},{"location":"Pipeline-Documentation/#pipelines-and-training","text":"Both web and desktop versions are capable of running canned pipelines and model training on your ground truth data. This document is to help you decide which pipeline to run.","title":"Pipelines and Training"},{"location":"Pipeline-Documentation/#help-me-choose","text":"Contact our team if you need help choosing the right data analysis strategy. Please upload some sample data to viame.kitware.com to allow us to better assist you.","title":"Help me choose"},{"location":"Pipeline-Documentation/#detection","text":"Best for a series of images that have no temporal relationship, such as arial photography of multiple scenes. Also preferred if you only care about aggregate data for the dataset, such as max occurrences of an object per scene. Pipeline Use case arctic seal eo yolo detector for color imagery arctic seal ir yolo detector for infrared em tuna detector for identifying individual features of tuna fish without motion simple single-class fish detector generic proposals generic object detector motion detect regions of motion in video or time-series images pengcam kw Penguin cam full-frame classifier pengcam swfsc Penguin cam full-frame classifier scallop and flatfish detector for benthic images scallop and flatfish left detector for benthic images, process left half of each frame only scallop netharn deep learning detector for benthic images scallop netharn left deep learning detector for benthic images, process left half of each frame only sea lion multi class detects bulls, cows, pups, etc sea lion single class detector sefsc bw group black-and-white fish detector (18 class, lower granularity) (oldest, v1) sefsc bw species v2 black-and-white fish species detector (updated, v2)","title":"Detection"},{"location":"Pipeline-Documentation/#tracking","text":"Run full tracking pipelines on your data. Appropriate for videos and image sequences that derive from a video. Tracking involves first running a detection pipeline then performing detection linking to form connected object tracks. Note some trackers can perform differently on time-series data depending on the annotation framerate selected when you upload or import your dataset. Higher framerates take longer to process, but may produce better results. Pipeline Use case em tuna tracker fish simple fish tracker fish.sfd tracker generic generic object tracker puts generic boxes around arbitrary objects motion identifies moving object tracks mouss tracker, trained with data from MOUSS (Modular Optical Underwater Survey System) sefsc bw * same as above, but with tracking","title":"Tracking"},{"location":"Pipeline-Documentation/#utility","text":"An assortment of other types of utility pipelines. Utility pipelines are named utility_<name>.pipe and are unique in that they may take detections as inputs (but are not required to). Pipeline Use case add segmentations watershed Transform existing bounding boxes into polygons empty frame lbls {N}fr Add an empty bounding box covering the whole media element for the purpose of adding full-frame classifier attributes. Unique tracks are created every N frames. track user selections Create tracks from user-initialized detection bounding boxes. Draw a box on the first frame of a track, and the pipeline will continue tracking the selected object(s)","title":"Utility"},{"location":"Pipeline-Documentation/#training","text":"Run model training on ground truth annotations. Currently, training configurations are available to do object detection, object classification, and full-frame classification. Tracker training will be added in a future update. Full-frame classifiers can be trained on arbitrary multi-class labels. It's helpful to start with empty frame lbls utility pipe and add type annotations to each generated frame. Object classifiers and detectors are trained on bounding boxes with arbitrary multi-class labels.","title":"Training"},{"location":"Pipeline-Documentation/#overview","text":"SVM ( Support Vector Machine ) configurations are usable with the smallest amount of ground-truth and train relatively quickly. NetHarn is a pytorch deep learning framework that requires more input data: on the order of thousands of target examples. There are two architectures used. Netharn models can take up to several days to train. Cascade Faster R-CNN (cfrnn) for training box detectors Mask R-CNN for training pixel classification and box detection ResNet (Residual Network) for training full frame or secondary object classifiers","title":"Overview"},{"location":"Pipeline-Documentation/#options","text":"","title":"Options"},{"location":"Pipeline-Documentation/#new-model-name","text":"A recognizable name for the pipeline that results from the training run.","title":"New Model Name"},{"location":"Pipeline-Documentation/#configuration-file","text":"One of the configuration options in the table below.","title":"Configuration File"},{"location":"Pipeline-Documentation/#labelstxt-file","text":"This optional file controls the output classes that a newly trained model will generate. Use if you annotated using higher granularity labels (such as species names) and want to train a classifier using more Or you want to restrict your training session to only train on certain kinds of ground-truth data. The following example labels.txt shows how to train a FISH classifier by combining redfish and bluefish , preserve the ROCK label, and omit every other label. 1 2 FISH redfish bluefish ROCK By default, all classes from all input datasets are preserved in the output model.","title":"Labels.txt file"},{"location":"Pipeline-Documentation/#use-annotation-frames-only","text":"By default, training runs include all frames from the chosen input datasets, and frames without annotations are considered negative examples. If you choose to use annotated frames only, frames or images with zero annotations will be discarded. This option is useful for trying to train on datasets that are only partially annotated.","title":"Use annotation frames only"},{"location":"Pipeline-Documentation/#configurations","text":"Configuration Availability Use Case detector_default both alias: train detector netharn cfrnn detector_netharn_cfrnn both detector_netharn_mask_rcnn both detector_svm_over_generic_detections both general purpose svm detector_svm_over_fish_detections both fish svm frame_classifier_default both alias: frame classifier netharn resnet frame_classifier_netharn_resnet both full-frame frame_classifier_svm_overn_resnet both full-frame object_classifier_default both alias: netharn resnet object classifier object_classifier_netharn_resnet both yolo desktop only can train, but resulting model is not runnable with desktop yet","title":"Configurations"},{"location":"Pipeline-Documentation/#pipeline-import-and-export","text":"Pipelines created outside of VIAME Web can be upload and shared with other users. See Pipeline Import and Export for details.","title":"Pipeline Import and Export"},{"location":"Pipeline-Import-Export/","text":"Pipeline Import and Export Trained model downloads You can download your trained models through the administrative interface. Warning Use caution when modifying data through the admin interface Open the admin interface at https://viame.kitware.com/girder (or myserver.com/girder if you host your own instance) Navigate to your personal workspace by clicking My Folders under your user dropdown in the top right corner. Navigate to the VIAME/VIAME Training Results folder and into the folder you wish to download Select all items and download using the menu Custom Pipeline Upload It's possible to upload custom pipes to DIVE Web through the girder interface. Warning This feature is not yet standardized, and the instructions below may change. Open the girder interface at /girder and create a new private folder called MyPipelines For our demo instance, open https://viame.kitware.com/girder Create a new folder in that private folder, and give it a name you'd like to associate with your new pipeline. Upload one or more files inside your new pipeline subfolder: A pipeline file ending in the .pipe file extension Whatever other model .zip files are required by the pipe, named exactly as they appear in your .pipe file above. Finally, set the pipeline folder metadata key trained_pipeline with value true . Your new pipeline will be available under the Run Pipeline -> Trained menu from the DIVE web app. Accepting input If your pipe must accept input, set the pipeline folder metadata property requires_input to true . Including base pipelines User-uploaded pipelines may depend on any pipe already installed from the base image or an addon using include <pipename>.pipe . Depending on other user-uploaded pipes is not supported. Tip KWIVER pipe files can be exported for use with DIVE using kwiver pipe-config","title":"Pipeline Import and Export"},{"location":"Pipeline-Import-Export/#pipeline-import-and-export","text":"","title":"Pipeline Import and Export"},{"location":"Pipeline-Import-Export/#trained-model-downloads","text":"You can download your trained models through the administrative interface. Warning Use caution when modifying data through the admin interface Open the admin interface at https://viame.kitware.com/girder (or myserver.com/girder if you host your own instance) Navigate to your personal workspace by clicking My Folders under your user dropdown in the top right corner. Navigate to the VIAME/VIAME Training Results folder and into the folder you wish to download Select all items and download using the menu","title":"Trained model downloads"},{"location":"Pipeline-Import-Export/#custom-pipeline-upload","text":"It's possible to upload custom pipes to DIVE Web through the girder interface. Warning This feature is not yet standardized, and the instructions below may change. Open the girder interface at /girder and create a new private folder called MyPipelines For our demo instance, open https://viame.kitware.com/girder Create a new folder in that private folder, and give it a name you'd like to associate with your new pipeline. Upload one or more files inside your new pipeline subfolder: A pipeline file ending in the .pipe file extension Whatever other model .zip files are required by the pipe, named exactly as they appear in your .pipe file above. Finally, set the pipeline folder metadata key trained_pipeline with value true . Your new pipeline will be available under the Run Pipeline -> Trained menu from the DIVE web app.","title":"Custom Pipeline Upload"},{"location":"Pipeline-Import-Export/#accepting-input","text":"If your pipe must accept input, set the pipeline folder metadata property requires_input to true .","title":"Accepting input"},{"location":"Pipeline-Import-Export/#including-base-pipelines","text":"User-uploaded pipelines may depend on any pipe already installed from the base image or an addon using include <pipename>.pipe . Depending on other user-uploaded pipes is not supported. Tip KWIVER pipe files can be exported for use with DIVE using kwiver pipe-config","title":"Including base pipelines"},{"location":"Screenshots/","text":"Screenshots This page provides a general overview of the differences between desktop and web through screenshots. Browse files Web Desktop Jobs List Web Desktop Annotator Web Desktop Training Config Web Desktop Settings Desktop","title":"Screenshots"},{"location":"Screenshots/#screenshots","text":"This page provides a general overview of the differences between desktop and web through screenshots.","title":"Screenshots"},{"location":"Screenshots/#browse-files","text":"Web Desktop","title":"Browse files"},{"location":"Screenshots/#jobs-list","text":"Web Desktop","title":"Jobs List"},{"location":"Screenshots/#annotator","text":"Web Desktop","title":"Annotator"},{"location":"Screenshots/#training-config","text":"Web Desktop","title":"Training Config"},{"location":"Screenshots/#settings","text":"Desktop","title":"Settings"},{"location":"Support/","text":"Support DIVE is free and open-source software published under an Apache 2.0 License in accordance with Kitware's Open Philosophy . Community Support For feedback, problems, questions, or feature requests, please reach out on Discourse. Our team would be happy to hear from you! Open a thread on Discourse Advanced Support If you or your employer have an active support contract with Kitware and you need individualized support, please email us directly at viame-web@kitware.com . Email us Demos or Custom Development DIVE is built and maintained by the Data & Analytics group at Kitware with financial support from contracts with government and commercial customers. If you would like to schedule a demo, sponsor custom software development, or purchase support related to DIVE, please email us or reach out through our contact page .","title":"Support"},{"location":"Support/#support","text":"DIVE is free and open-source software published under an Apache 2.0 License in accordance with Kitware's Open Philosophy .","title":"Support"},{"location":"Support/#community-support","text":"For feedback, problems, questions, or feature requests, please reach out on Discourse. Our team would be happy to hear from you! Open a thread on Discourse","title":"Community Support"},{"location":"Support/#advanced-support","text":"If you or your employer have an active support contract with Kitware and you need individualized support, please email us directly at viame-web@kitware.com . Email us","title":"Advanced Support"},{"location":"Support/#demos-or-custom-development","text":"DIVE is built and maintained by the Data & Analytics group at Kitware with financial support from contracts with government and commercial customers. If you would like to schedule a demo, sponsor custom software development, or purchase support related to DIVE, please email us or reach out through our contact page .","title":"Demos or Custom Development"},{"location":"UI-Annotation-View/","text":"Annotation Window The annotation window will look different based on the current mode and what visibility toggles are enabled. Left Click an annotation to select it. Right Click an annotation to select it and enter editing mode. Middle Click and drag to pan the camera. This is useful when drawing annotations while zoomed such that you need to work on something slightly off-screen. Viewer modes Default Mode - In the default mode the annotation will have bounds associated with it as well as a text name for the type and an associated confidence level. The color and styling will match what is specified in the Type List Style Settings . There are additional modes which can be toggled on and off in the Edit Bar . Selected Annotation - selected annotations are cyan in color Editing Annotation - Editing annotations are cyan in color and provide handles to resize the annotation as well as a central handle to move the annotation to different spot. Creating Annotation - Creating an annotation requires clicking and dragging the mouse. Creating in the annotation window is indicated by a cursor crosshair and an icon that shows the type being drawn. Interpolated Annotation - If a track has an interpolated box on the current frame it will appear slightly faded.","title":"Annotation Window"},{"location":"UI-Annotation-View/#annotation-window","text":"The annotation window will look different based on the current mode and what visibility toggles are enabled. Left Click an annotation to select it. Right Click an annotation to select it and enter editing mode. Middle Click and drag to pan the camera. This is useful when drawing annotations while zoomed such that you need to work on something slightly off-screen.","title":"Annotation Window"},{"location":"UI-Annotation-View/#viewer-modes","text":"Default Mode - In the default mode the annotation will have bounds associated with it as well as a text name for the type and an associated confidence level. The color and styling will match what is specified in the Type List Style Settings . There are additional modes which can be toggled on and off in the Edit Bar . Selected Annotation - selected annotations are cyan in color Editing Annotation - Editing annotations are cyan in color and provide handles to resize the annotation as well as a central handle to move the annotation to different spot. Creating Annotation - Creating an annotation requires clicking and dragging the mouse. Creating in the annotation window is indicated by a cursor crosshair and an icon that shows the type being drawn. Interpolated Annotation - If a track has an interpolated box on the current frame it will appear slightly faded.","title":"Viewer modes"},{"location":"UI-Attributes/","text":"Attributes Concepts and Terms Attribute Definitions are templates. They have a name and a value type, such as String , Number , or Boolean . Definitions must be created before attribute values can be assigned. Tracks and detections each have their own set of definitions. Track Attributes apply to an entire track. Each track can only have one value for each track attribute definition. Detection Attributes can be different for every frame in a track. Example Attribute Definition Track Attributes: CompleteTrack: Boolean FishLength: number (cm) Detection Attributes: Swimming: Boolean Eating: Boolean Example Attribute Values Fish Track 1 Track Attributes { \"FishLength\": 20 } Detection Attributes Frame 1 { \"Eating\": True } Frame 2 { \"Swimming\": False, \"Eating\": True } Info All Attribute definitions do not need to be assigned to values. CompleteTrack (Track Attribute) and Swimming for Frame 1 (Detection Attribute) weren't assigned in this example. Using the Attributes Panel Select an existing track or detection with left click. Open the Track Details page by clicking on the button in the Type List area or pressing the A key. Here you will see the track/detection type, confidence pairs associated with it and then a list of track and detection attributes. For attributes there are two sections Track Attributes - All track level attributes Detection Attributes - attributes associated with the track on a per frame basis Info Attributes found during import in a VIAME CSV will automatically show up in the list. The data type of the attribute is guessed by examining values and may need to be manually corrected. By default, all attributes associated with the dataset are visible and editable. You can hide unused attributes by clicking the toggle next to Attribute . Show Unused Hide Unused Creating Attribute Definitions Click on the Attribute icon for in either the track or detection attribute area. Enter a unique name Choose a Datatype Number Boolean (True/False) Text Custom text that the user provides A predefined list of text to choose from, separated by newline. Click Save to add the new attribute Editing Attribute Definitions Click the button next to an existing attribute to edit its definition. Warning Editing or deleting an attribute definition doesn\u2019t change any existing attribute values . Deleting an attribute definition will cause it to disappear from the list, but the values will remain in the database. Editing an attribute definition will change the way the controls behave, but will not change any existing set values. Setting Attribute Values Click on the attribute value when in viewing mode to edit and set the attribute Or directly edit the value field when in the attribute editing mode Setting an attribute to the empty value will remove the value from the track/detection Importing and Exporting Attributes Attributes are part of the dataset configuration that can be imported and exported. Set up a dataset with all the attributes you need In the Download menu, choose Configuration . Use this configuration with other datasets Use the Import button to load this configuration to other datasets. Upload the configuration file when you create new datasets to initialize them with these attribute definitions. Applying Attributes Demo","title":"Attributes"},{"location":"UI-Attributes/#attributes","text":"","title":"Attributes"},{"location":"UI-Attributes/#concepts-and-terms","text":"Attribute Definitions are templates. They have a name and a value type, such as String , Number , or Boolean . Definitions must be created before attribute values can be assigned. Tracks and detections each have their own set of definitions. Track Attributes apply to an entire track. Each track can only have one value for each track attribute definition. Detection Attributes can be different for every frame in a track.","title":"Concepts and Terms"},{"location":"UI-Attributes/#example-attribute-definition","text":"Track Attributes: CompleteTrack: Boolean FishLength: number (cm) Detection Attributes: Swimming: Boolean Eating: Boolean","title":"Example Attribute Definition"},{"location":"UI-Attributes/#example-attribute-values","text":"Fish Track 1 Track Attributes { \"FishLength\": 20 } Detection Attributes Frame 1 { \"Eating\": True } Frame 2 { \"Swimming\": False, \"Eating\": True } Info All Attribute definitions do not need to be assigned to values. CompleteTrack (Track Attribute) and Swimming for Frame 1 (Detection Attribute) weren't assigned in this example.","title":"Example Attribute Values"},{"location":"UI-Attributes/#using-the-attributes-panel","text":"Select an existing track or detection with left click. Open the Track Details page by clicking on the button in the Type List area or pressing the A key. Here you will see the track/detection type, confidence pairs associated with it and then a list of track and detection attributes. For attributes there are two sections Track Attributes - All track level attributes Detection Attributes - attributes associated with the track on a per frame basis Info Attributes found during import in a VIAME CSV will automatically show up in the list. The data type of the attribute is guessed by examining values and may need to be manually corrected. By default, all attributes associated with the dataset are visible and editable. You can hide unused attributes by clicking the toggle next to Attribute . Show Unused Hide Unused","title":"Using the Attributes Panel"},{"location":"UI-Attributes/#creating-attribute-definitions","text":"Click on the Attribute icon for in either the track or detection attribute area. Enter a unique name Choose a Datatype Number Boolean (True/False) Text Custom text that the user provides A predefined list of text to choose from, separated by newline. Click Save to add the new attribute","title":"Creating Attribute Definitions"},{"location":"UI-Attributes/#editing-attribute-definitions","text":"Click the button next to an existing attribute to edit its definition. Warning Editing or deleting an attribute definition doesn\u2019t change any existing attribute values . Deleting an attribute definition will cause it to disappear from the list, but the values will remain in the database. Editing an attribute definition will change the way the controls behave, but will not change any existing set values.","title":"Editing Attribute Definitions"},{"location":"UI-Attributes/#setting-attribute-values","text":"Click on the attribute value when in viewing mode to edit and set the attribute Or directly edit the value field when in the attribute editing mode Setting an attribute to the empty value will remove the value from the track/detection","title":"Setting Attribute Values"},{"location":"UI-Attributes/#importing-and-exporting-attributes","text":"Attributes are part of the dataset configuration that can be imported and exported. Set up a dataset with all the attributes you need In the Download menu, choose Configuration . Use this configuration with other datasets Use the Import button to load this configuration to other datasets. Upload the configuration file when you create new datasets to initialize them with these attribute definitions.","title":"Importing and Exporting Attributes"},{"location":"UI-Attributes/#applying-attributes-demo","text":"","title":"Applying Attributes Demo"},{"location":"UI-Navigation-Editing-Bar/","text":"Navigation Bar The navigation bar is the row of controls at the very top of the window. Data navigates to the folder that contains the current dataset. Run Pipeline will launch a pipeline dropdown menu. NOTE Current annotations will be replaced by the pipeline output when it is complete. You should not perform annotations while a pipeline is running. Import allows the upload of several kinds of files overwrite the current annotations with a .json or .csv annotation file. overwrite the style and attribute configuration with a config .json file. Download (Web) or Export (Desktop) allows for exporting all or part of the current dataset. Exclude Tracks - this allows you to remove tracks below a specific confidence threshold when exporting the CSV. It is how you can export only the higher detections/tracks after running a pipeline. Checked Types Only - allows you to only export the annotations of types that are currently checked in the type list. Web-specific options are documented in the web download section Clone is documented in the web clone section . Help provides mouse/keyboard shortcuts as well as a link to this documentation. is used to save outstanding annotation changes and any custom styles applied to the different types. Changes are not immediately committed and will instead update the save icon with a number badge indicating how many changes are outstanding. Clicking this button will commit your changes and reset the count to zero. Editing Bar The editing bar is the second row below navigation. Visibility Toggles The visibility section contains toggle buttons that control the different types of annotation data can be hidden or shown. toggles rectangle visibility toggles polygon visibility toggles head/tail line visibility toggles annotation type & confidence text visibility toggles a cursor hover tooltip , helpful for reviewing very dense scenes with lots of overlap. toggles track trail visibility. The track trail is configurable to show up to 100 frames both ahead and behind each bounding box. The trail line is made of bounding box midpoints. Edit Mode Toggles Editing mode toggles control the type of geometry being created or edited during annotation. See the Annotation Quickstart for an in-depth guide to annotation.","title":"Navigation and Editing Bar"},{"location":"UI-Navigation-Editing-Bar/#navigation-bar","text":"The navigation bar is the row of controls at the very top of the window. Data navigates to the folder that contains the current dataset. Run Pipeline will launch a pipeline dropdown menu. NOTE Current annotations will be replaced by the pipeline output when it is complete. You should not perform annotations while a pipeline is running. Import allows the upload of several kinds of files overwrite the current annotations with a .json or .csv annotation file. overwrite the style and attribute configuration with a config .json file. Download (Web) or Export (Desktop) allows for exporting all or part of the current dataset. Exclude Tracks - this allows you to remove tracks below a specific confidence threshold when exporting the CSV. It is how you can export only the higher detections/tracks after running a pipeline. Checked Types Only - allows you to only export the annotations of types that are currently checked in the type list. Web-specific options are documented in the web download section Clone is documented in the web clone section . Help provides mouse/keyboard shortcuts as well as a link to this documentation. is used to save outstanding annotation changes and any custom styles applied to the different types. Changes are not immediately committed and will instead update the save icon with a number badge indicating how many changes are outstanding. Clicking this button will commit your changes and reset the count to zero.","title":"Navigation Bar"},{"location":"UI-Navigation-Editing-Bar/#editing-bar","text":"The editing bar is the second row below navigation.","title":"Editing Bar"},{"location":"UI-Navigation-Editing-Bar/#visibility-toggles","text":"The visibility section contains toggle buttons that control the different types of annotation data can be hidden or shown. toggles rectangle visibility toggles polygon visibility toggles head/tail line visibility toggles annotation type & confidence text visibility toggles a cursor hover tooltip , helpful for reviewing very dense scenes with lots of overlap. toggles track trail visibility. The track trail is configurable to show up to 100 frames both ahead and behind each bounding box. The trail line is made of bounding box midpoints.","title":"Visibility Toggles"},{"location":"UI-Navigation-Editing-Bar/#edit-mode-toggles","text":"Editing mode toggles control the type of geometry being created or edited during annotation. See the Annotation Quickstart for an in-depth guide to annotation.","title":"Edit Mode Toggles"},{"location":"UI-Timeline/","text":"Timeline The timeline provides a control bar and a few different temporal visualizations. All timeline visualizations are updated live by type confidence slider(s), type checkboxes, and track checkboxes. Control Bar will minimize the timeline. Detections button selects the Detection Count histogram timeline view. Events button selects the Event View , which is a Gantt-style track chart. are standard media playback controls. will enable camera lock, which causes the annotation view to auto-zoom and pan to whatever annotation is currently selected. This is useful when reviewing the output of a pipeline. or the R key will reset zoom/pan in the annotation view. Detection Count This is the default visualization. It is a stacked histogram of track/detection types over the duration of the sequence. Line color matches the annotation type style. Top green line is the sum count of all annotations of all types on each frame. Event View The event viewer shows the start/stop frames for all tracks. It is a kind of Gantt chart, also similar to a swimlane chart but with more compact packing. The tracks are drawn using their corresponding type color. When hovering over any track the TrackID will display. Clicking on a track will select it and jump to the track at the selected frame. Interpreting Single frame detections are presented as single frames with spaces between. A selected track will be cyan and will cause all other tracks to fade out. If a selected track is solid cyan, that means every frame in the track is a keyframe. A selected interpolated track will show the areas of interpolation as yellow lines, the keyframes as cyan ticks, and gaps as empty regions.","title":"Timeline"},{"location":"UI-Timeline/#timeline","text":"The timeline provides a control bar and a few different temporal visualizations. All timeline visualizations are updated live by type confidence slider(s), type checkboxes, and track checkboxes.","title":"Timeline"},{"location":"UI-Timeline/#control-bar","text":"will minimize the timeline. Detections button selects the Detection Count histogram timeline view. Events button selects the Event View , which is a Gantt-style track chart. are standard media playback controls. will enable camera lock, which causes the annotation view to auto-zoom and pan to whatever annotation is currently selected. This is useful when reviewing the output of a pipeline. or the R key will reset zoom/pan in the annotation view.","title":"Control Bar"},{"location":"UI-Timeline/#detection-count","text":"This is the default visualization. It is a stacked histogram of track/detection types over the duration of the sequence. Line color matches the annotation type style. Top green line is the sum count of all annotations of all types on each frame.","title":"Detection Count"},{"location":"UI-Timeline/#event-view","text":"The event viewer shows the start/stop frames for all tracks. It is a kind of Gantt chart, also similar to a swimlane chart but with more compact packing. The tracks are drawn using their corresponding type color. When hovering over any track the TrackID will display. Clicking on a track will select it and jump to the track at the selected frame.","title":"Event View"},{"location":"UI-Timeline/#interpreting","text":"Single frame detections are presented as single frames with spaces between. A selected track will be cyan and will cause all other tracks to fade out. If a selected track is solid cyan, that means every frame in the track is a keyframe. A selected interpolated track will show the areas of interpolation as yellow lines, the keyframes as cyan ticks, and gaps as empty regions.","title":"Interpreting"},{"location":"UI-Track-List/","text":"Track List Track List Controls The track list allows for selecting and editing tracks. A selected track will look different depending on whether it's a single detection or a multi-frame track. opens track creation settings deletes all tracks in the track list Track/Detection begins creation of a new annotation. Single Detection A track that spans a single frame. deletes the entire detection annotation goes to the first frame of the detection selects the detection and toggles edit mode. Warning The button will remove the whole track if it's longer than a single detection. To remove individual keyframes, use (the keyframe toggle button). Multi-frame track A track that spans multiple frames and has more options deletes the entire track splits the track into 2 smaller tracks on the current frame. is filled in if the current frame annotation is a keyfame. Clicking this will either remove the keyframe if it exists or make the current interpolated annotation a keyframe. turns interpolation on/off for the interval between keyframes. jumps to the first frame of track jumps to the previous keyframe jumps to the next keyframe jumps to the last frame of the track selects the detection and toggles edit mode.","title":"Track List"},{"location":"UI-Track-List/#track-list","text":"","title":"Track List"},{"location":"UI-Track-List/#track-list-controls","text":"The track list allows for selecting and editing tracks. A selected track will look different depending on whether it's a single detection or a multi-frame track. opens track creation settings deletes all tracks in the track list Track/Detection begins creation of a new annotation.","title":"Track List Controls"},{"location":"UI-Track-List/#single-detection","text":"A track that spans a single frame. deletes the entire detection annotation goes to the first frame of the detection selects the detection and toggles edit mode. Warning The button will remove the whole track if it's longer than a single detection. To remove individual keyframes, use (the keyframe toggle button).","title":"Single Detection"},{"location":"UI-Track-List/#multi-frame-track","text":"A track that spans multiple frames and has more options deletes the entire track splits the track into 2 smaller tracks on the current frame. is filled in if the current frame annotation is a keyfame. Clicking this will either remove the keyframe if it exists or make the current interpolated annotation a keyframe. turns interpolation on/off for the interval between keyframes. jumps to the first frame of track jumps to the previous keyframe jumps to the next keyframe jumps to the last frame of the track selects the detection and toggles edit mode.","title":"Multi-frame track"},{"location":"UI-Type-List/","text":"Type List Type List Controls Each dataset maintains its own list of types, and types can be defined on-the-fly. The Type List is used to control visual styles of the different types as well as filter out types that don't need to be displayed. The checkbox next to each type name can be used to toggle types on and off. toggles the sort order between alphabetical and by number of annotations of each type. opens the type settings menu. will remove the type from any visible track or delete the track if it is the only type. will switch the left sidebar panel to show the track attribute editor view. Confidence filtering is documented on the confidence filter page Type Style Editor The type style editor controls the visual appearance of annotations in all other areas of the application. Launch the editor by hovering over a type row in the list and clicking (the edit pencil). Type Name - You can change the name for the type and it will update all subsequent tracks that are using that Type. Show Label - show the type name label in the text above each box. Show Confidence - show the confidence value in the text above each box. Box Border Thickness - the line thickness can be changed to make a type stand out more or less Fill - Fill allows the bounding box to be filled. This is useful for annotation of background items in an image. Border & Fill Opacity - The opacity of the lines and the fill can be set here. Color - The color for the type within the annotations and the timeline views. Type Settings Menu Click the button in the type list heading to open type settings. Ad-hoc mode In ad-hoc mode, new object classes are added as you annotate. The type list updates automatically when new classes are added or the last member of a class is deleted. Set Lock Types to off for ad-hoc type creation. Set Show Empty to still show manually defined types with no track/detection examples in the type list. Locked mode In locked mode, only a specified list of classes can be used, and must be selected or autocompleted from the list for each object. Set Lock Types to on to constrain annotation types to those already defined. You can add new types using the Types button under type settings.","title":"Type List"},{"location":"UI-Type-List/#type-list","text":"","title":"Type List"},{"location":"UI-Type-List/#type-list-controls","text":"Each dataset maintains its own list of types, and types can be defined on-the-fly. The Type List is used to control visual styles of the different types as well as filter out types that don't need to be displayed. The checkbox next to each type name can be used to toggle types on and off. toggles the sort order between alphabetical and by number of annotations of each type. opens the type settings menu. will remove the type from any visible track or delete the track if it is the only type. will switch the left sidebar panel to show the track attribute editor view. Confidence filtering is documented on the confidence filter page","title":"Type List Controls"},{"location":"UI-Type-List/#type-style-editor","text":"The type style editor controls the visual appearance of annotations in all other areas of the application. Launch the editor by hovering over a type row in the list and clicking (the edit pencil). Type Name - You can change the name for the type and it will update all subsequent tracks that are using that Type. Show Label - show the type name label in the text above each box. Show Confidence - show the confidence value in the text above each box. Box Border Thickness - the line thickness can be changed to make a type stand out more or less Fill - Fill allows the bounding box to be filled. This is useful for annotation of background items in an image. Border & Fill Opacity - The opacity of the lines and the fill can be set here. Color - The color for the type within the annotations and the timeline views.","title":"Type Style Editor"},{"location":"UI-Type-List/#type-settings-menu","text":"Click the button in the type list heading to open type settings.","title":"Type Settings Menu"},{"location":"UI-Type-List/#ad-hoc-mode","text":"In ad-hoc mode, new object classes are added as you annotate. The type list updates automatically when new classes are added or the last member of a class is deleted. Set Lock Types to off for ad-hoc type creation. Set Show Empty to still show manually defined types with no track/detection examples in the type list.","title":"Ad-hoc mode"},{"location":"UI-Type-List/#locked-mode","text":"In locked mode, only a specified list of classes can be used, and must be selected or autocompleted from the list for each object. Set Lock Types to on to constrain annotation types to those already defined. You can add new types using the Types button under type settings.","title":"Locked mode"},{"location":"Web-Version/","text":"Info VIAME Web is automatically updated at 2AM EST/EDT every Thursday. If you are running a pipeline or training workflow during update, it will be interrupted and restarted. Also note that pipelines and training jobs on our public server are limited to 3 days of execution time on 1 GPU. If you have a job that needs more time, please run it with the Desktop version, your own cloud environment like GCP, or contact us for support . Use our public server Deploy your own Register for an account A user account is required to store data and run pipelines on viame.kitware.com. Visit viame.kitware.com Click Register Uploading data Uploading individual files Open the DIVE Homepage, and navigate to the Data tab. Click the User Home button at the top left of the data browser. Click either your Public or Private folder, or make a new folder and navigate into it. Click the Upload button that appears in the toolbar. Select a video or multi-select a group of image frames. Use Ctrl or Shift to click every file you want to upload. If you already have annotations.csv or an annotation or configuration JSON select that too. Choose a name for the dataset and enter the optional playback frame rate or select other optional files. Press Start Upload In the data browser, a new Launch Annotator button will appear next to your data If you uploaded a video, it may need to transcode first Info All video uploaded to the web server will be transcoded as mp4/h264 . Uploading zip files A zip import can have one of the following file combinations: One or more images, an optional annotation file, and an optional configuration file One video with an optional annotation file and an optional configuration file One or more folders which contain the above examples (These will be converted to separate datasets) Zip import also accepts zip archive files that were generated by the Download Everything export button. Download or export data Data can be downloaded from the FileBrowser by clicking the checkmark to the left of a dataset name. This allows you to download the source images/video, the current detection file converted to .csv or everything including all backups of the detection files. Image Sequence or Video will export the source media as a .zip Detections will export a VIAME .csv of annotations Checkbox options are explained in the Navigation Bar Section . Configuration will export a DIVE configuration .json Everything will export all of the above. Sharing data with teams This information will be relevant to teams where several people need to work on the same data. Concepts By default, data uploaded to your personal user space follows these conventions. Data in the Public folder is readable by all registered users, but writable only by you by default. Data in the Private folder is only visible to you by default. Working with teams A common scenario is for a group to have a lot of shared data that several members should be able to view and annotate. For most teams, we recommend keeping data consolidated under a single account then following the sharing instructions below to make sure all team members have appropriate access. It's easiest to create a single parent folder to share and then put all individual datasets inside that parent. Warning You should note that 2 people cannot work on the same video at the same time. Your team should coordinate on who will work on each dataset. Managing Permissions DIVE uses Girder's Permissions Model . There are four levels of permission a User can have on a resource. No permission (cannot view, edit, or delete a resource) READ permission (can view and download resources) WRITE permission (includes READ permission, can edit the properties of a resource) ADMIN also known as own permission, (includes READ and WRITE permission, can delete the resource and also control access on it) Granting access to others Navigate to your data in the data browser. Right click a dataset or a folder of datasets and choose Access Control Search for and select users you want to grant access to. Select the correct permissions in the drop-down next to each user. If this is a folder of datasets, enable the Include Subfolders switch. Click Save . These users should now be able to view and edit your data. Data Shared with you You can view data shared with you by selecting the Shared With Me tab above the data browser. Sharing URLs You can copy and paste any URL from the address bar and share with collaborators. This includes folders in the data browser as well as direct links to the annotation editor. Dataset Clones A clone is a shallow copy of a dataset. It has its own annotations, and can be run through pipelines and shared with others. It references the media (images or video) of another dataset. Warning Be careful when deleting data that has been cloned. Clones \"point to\" their source dataset for loading media, so if the source is deleted, all of its clones will fail to load. Clone use cases When you want to use or modify data that doesn't belong to you, such as data from the shared training collection or from other users. When you want to run several different pipelines in parallel on the same input data and compare the results. Warning Merging cloned data back to the source is not currently supported . To collaborate with others on annotations, the sharing use case above is preferred. How to clone Open the dataset you wish to clone by clicking Launch Annotator . Click the Clone button in the top navigation bar on the right side. Choose a name and location for the clone within your own workspace.","title":"Web Version"},{"location":"Web-Version/#register-for-an-account","text":"A user account is required to store data and run pipelines on viame.kitware.com. Visit viame.kitware.com Click Register","title":"Register for an account"},{"location":"Web-Version/#uploading-data","text":"","title":"Uploading data"},{"location":"Web-Version/#uploading-individual-files","text":"Open the DIVE Homepage, and navigate to the Data tab. Click the User Home button at the top left of the data browser. Click either your Public or Private folder, or make a new folder and navigate into it. Click the Upload button that appears in the toolbar. Select a video or multi-select a group of image frames. Use Ctrl or Shift to click every file you want to upload. If you already have annotations.csv or an annotation or configuration JSON select that too. Choose a name for the dataset and enter the optional playback frame rate or select other optional files. Press Start Upload In the data browser, a new Launch Annotator button will appear next to your data If you uploaded a video, it may need to transcode first Info All video uploaded to the web server will be transcoded as mp4/h264 .","title":"Uploading individual files"},{"location":"Web-Version/#uploading-zip-files","text":"A zip import can have one of the following file combinations: One or more images, an optional annotation file, and an optional configuration file One video with an optional annotation file and an optional configuration file One or more folders which contain the above examples (These will be converted to separate datasets) Zip import also accepts zip archive files that were generated by the Download Everything export button.","title":"Uploading zip files"},{"location":"Web-Version/#download-or-export-data","text":"Data can be downloaded from the FileBrowser by clicking the checkmark to the left of a dataset name. This allows you to download the source images/video, the current detection file converted to .csv or everything including all backups of the detection files. Image Sequence or Video will export the source media as a .zip Detections will export a VIAME .csv of annotations Checkbox options are explained in the Navigation Bar Section . Configuration will export a DIVE configuration .json Everything will export all of the above.","title":"Download or export data"},{"location":"Web-Version/#sharing-data-with-teams","text":"This information will be relevant to teams where several people need to work on the same data.","title":"Sharing data with teams"},{"location":"Web-Version/#concepts","text":"By default, data uploaded to your personal user space follows these conventions. Data in the Public folder is readable by all registered users, but writable only by you by default. Data in the Private folder is only visible to you by default.","title":"Concepts"},{"location":"Web-Version/#working-with-teams","text":"A common scenario is for a group to have a lot of shared data that several members should be able to view and annotate. For most teams, we recommend keeping data consolidated under a single account then following the sharing instructions below to make sure all team members have appropriate access. It's easiest to create a single parent folder to share and then put all individual datasets inside that parent. Warning You should note that 2 people cannot work on the same video at the same time. Your team should coordinate on who will work on each dataset.","title":"Working with teams"},{"location":"Web-Version/#managing-permissions","text":"DIVE uses Girder's Permissions Model . There are four levels of permission a User can have on a resource. No permission (cannot view, edit, or delete a resource) READ permission (can view and download resources) WRITE permission (includes READ permission, can edit the properties of a resource) ADMIN also known as own permission, (includes READ and WRITE permission, can delete the resource and also control access on it)","title":"Managing Permissions"},{"location":"Web-Version/#granting-access-to-others","text":"Navigate to your data in the data browser. Right click a dataset or a folder of datasets and choose Access Control Search for and select users you want to grant access to. Select the correct permissions in the drop-down next to each user. If this is a folder of datasets, enable the Include Subfolders switch. Click Save . These users should now be able to view and edit your data.","title":"Granting access to others"},{"location":"Web-Version/#data-shared-with-you","text":"You can view data shared with you by selecting the Shared With Me tab above the data browser.","title":"Data Shared with you"},{"location":"Web-Version/#sharing-urls","text":"You can copy and paste any URL from the address bar and share with collaborators. This includes folders in the data browser as well as direct links to the annotation editor.","title":"Sharing URLs"},{"location":"Web-Version/#dataset-clones","text":"A clone is a shallow copy of a dataset. It has its own annotations, and can be run through pipelines and shared with others. It references the media (images or video) of another dataset. Warning Be careful when deleting data that has been cloned. Clones \"point to\" their source dataset for loading media, so if the source is deleted, all of its clones will fail to load.","title":"Dataset Clones"},{"location":"Web-Version/#clone-use-cases","text":"When you want to use or modify data that doesn't belong to you, such as data from the shared training collection or from other users. When you want to run several different pipelines in parallel on the same input data and compare the results. Warning Merging cloned data back to the source is not currently supported . To collaborate with others on annotations, the sharing use case above is preferred.","title":"Clone use cases"},{"location":"Web-Version/#how-to-clone","text":"Open the dataset you wish to clone by clicking Launch Annotator . Click the Clone button in the top navigation bar on the right side. Choose a name and location for the clone within your own workspace.","title":"How to clone"}]}